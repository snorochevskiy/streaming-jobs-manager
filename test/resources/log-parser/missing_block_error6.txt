Ivy Default Cache set to: /home/hadoop/.ivy2/cache
The jars for the packages stored in: /home/hadoop/.ivy2/jars
:: loading settings :: url = jar:file:/usr/lib/spark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-c2caeee9-70e3-41e6-a835-238485a1a670;1.0
	confs: [default]
	found org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 in central
	found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 in central
	found org.apache.kafka#kafka-clients;2.4.1 in central
	found com.github.luben#zstd-jni;1.4.4-3 in central
	found org.lz4#lz4-java;1.7.1 in central
	found org.xerial.snappy#snappy-java;1.1.7.5 in central
	found org.slf4j#slf4j-api;1.7.30 in central
	found org.spark-project.spark#unused;1.0.0 in central
	found org.apache.commons#commons-pool2;2.6.2 in central
:: resolution report :: resolve 398ms :: artifacts dl 9ms
	:: modules in use:
	com.github.luben#zstd-jni;1.4.4-3 from central in [default]
	org.apache.commons#commons-pool2;2.6.2 from central in [default]
	org.apache.kafka#kafka-clients;2.4.1 from central in [default]
	org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 from central in [default]
	org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 from central in [default]
	org.lz4#lz4-java;1.7.1 from central in [default]
	org.slf4j#slf4j-api;1.7.30 from central in [default]
	org.spark-project.spark#unused;1.0.0 from central in [default]
	org.xerial.snappy#snappy-java;1.1.7.5 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   9   |   0   |   0   |   0   ||   9   |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-c2caeee9-70e3-41e6-a835-238485a1a670
	confs: [default]
	0 artifacts copied, 9 already retrieved (0kB/11ms)
21/04/06 16:06:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/04/06 16:06:13 INFO RMProxy: Connecting to ResourceManager at ip-96-113-30-160.nest.r53.xcal.tv/96.113.30.160:8032
21/04/06 16:06:13 INFO Client: Requesting a new application from cluster with 16 NodeManagers
21/04/06 16:06:14 INFO Configuration: resource-types.xml not found
21/04/06 16:06:14 INFO ResourceUtils: Unable to find 'resource-types.xml'.
21/04/06 16:06:14 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (57344 MB per container)
21/04/06 16:06:14 INFO Client: Will allocate AM container, with 4505 MB memory including 409 MB overhead
21/04/06 16:06:14 INFO Client: Setting up container launch context for our AM
21/04/06 16:06:14 INFO Client: Setting up the launch environment for our AM container
21/04/06 16:06:14 INFO Client: Preparing resources for our AM container
21/04/06 16:06:14 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
21/04/06 16:06:16 INFO Client: Uploading resource file:/mnt/tmp/spark-ab9fea9a-9977-410b-887a-b95924da238b/__spark_libs__5978304132716469213.zip -> hdfs://ip-96-113-30-160.nest.r53.xcal.tv:8020/user/hadoop/.sparkStaging/application_1615924390842_1222/__spark_libs__5978304132716469213.zip
21/04/06 16:06:16 INFO Client: Uploading resource file:/home/hadoop/streaming-jobs-.snmp-assembly-2.0.0.jar -> hdfs://ip-96-113-30-160.nest.r53.xcal.tv:8020/user/hadoop/.sparkStaging/application_1615924390842_1222/streaming-jobs-.snmp-assembly-2.0.0.jar
21/04/06 16:06:16 INFO Client: Uploading resource file:/home/hadoop/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.0.0.jar -> hdfs://ip-96-113-30-160.nest.r53.xcal.tv:8020/user/hadoop/.sparkStaging/application_1615924390842_1222/org.apache.spark_spark-sql-kafka-0-10_2.12-3.0.0.jar
21/04/06 16:06:16 INFO Client: Uploading resource file:/home/hadoop/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.0.0.jar -> hdfs://ip-96-113-30-160.nest.r53.xcal.tv:8020/user/hadoop/.sparkStaging/application_1615924390842_1222/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.0.0.jar
21/04/06 16:06:16 INFO Client: Uploading resource file:/home/hadoop/.ivy2/jars/org.apache.kafka_kafka-clients-2.4.1.jar -> hdfs://ip-96-113-30-160.nest.r53.xcal.tv:8020/user/hadoop/.sparkStaging/application_1615924390842_1222/org.apache.kafka_kafka-clients-2.4.1.jar
21/04/06 16:06:16 INFO Client: Uploading resource file:/home/hadoop/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar -> hdfs://ip-96-113-30-160.nest.r53.xcal.tv:8020/user/hadoop/.sparkStaging/application_1615924390842_1222/org.apache.commons_commons-pool2-2.6.2.jar
21/04/06 16:06:16 INFO Client: Uploading resource file:/home/hadoop/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar -> hdfs://ip-96-113-30-160.nest.r53.xcal.tv:8020/user/hadoop/.sparkStaging/application_1615924390842_1222/org.spark-project.spark_unused-1.0.0.jar
21/04/06 16:06:17 INFO Client: Uploading resource file:/home/hadoop/.ivy2/jars/com.github.luben_zstd-jni-1.4.4-3.jar -> hdfs://ip-96-113-30-160.nest.r53.xcal.tv:8020/user/hadoop/.sparkStaging/application_1615924390842_1222/com.github.luben_zstd-jni-1.4.4-3.jar
21/04/06 16:06:17 INFO Client: Uploading resource file:/home/hadoop/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar -> hdfs://ip-96-113-30-160.nest.r53.xcal.tv:8020/user/hadoop/.sparkStaging/application_1615924390842_1222/org.lz4_lz4-java-1.7.1.jar
21/04/06 16:06:17 INFO Client: Uploading resource file:/home/hadoop/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar -> hdfs://ip-96-113-30-160.nest.r53.xcal.tv:8020/user/hadoop/.sparkStaging/application_1615924390842_1222/org.xerial.snappy_snappy-java-1.1.7.5.jar
21/04/06 16:06:18 INFO Client: Uploading resource file:/home/hadoop/.ivy2/jars/org.slf4j_slf4j-api-1.7.30.jar -> hdfs://ip-96-113-30-160.nest.r53.xcal.tv:8020/user/hadoop/.sparkStaging/application_1615924390842_1222/org.slf4j_slf4j-api-1.7.30.jar
21/04/06 16:06:18 INFO Client: Uploading resource file:/mnt/tmp/spark-ab9fea9a-9977-410b-887a-b95924da238b/__spark_conf__1086922750471331147.zip -> hdfs://ip-96-113-30-160.nest.r53.xcal.tv:8020/user/hadoop/.sparkStaging/application_1615924390842_1222/__spark_conf__.zip
21/04/06 16:06:18 INFO SecurityManager: Changing view acls to: hadoop
21/04/06 16:06:18 INFO SecurityManager: Changing modify acls to: hadoop
21/04/06 16:06:18 INFO SecurityManager: Changing view acls groups to:
21/04/06 16:06:18 INFO SecurityManager: Changing modify acls groups to:
21/04/06 16:06:18 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hadoop); groups with view permissions: Set(); users  with modify permissions: Set(hadoop); groups with modify permissions: Set()
21/04/06 16:06:18 INFO Client: Submitting application application_1615924390842_1222 to ResourceManager
21/04/06 16:06:18 INFO YarnClientImpl: Submitted application application_1615924390842_1222
21/04/06 16:06:19 INFO Client: Application report for application_1615924390842_1222 (state: ACCEPTED)
21/04/06 16:06:19 INFO Client:
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1617725178774
	 final status: UNDEFINED
	 tracking URL: http://ip-96-113-30-160.nest.r53.xcal.tv:20888/proxy/application_1615924390842_1222/
	 user: hadoop
21/04/06 16:06:20 INFO Client: Application report for application_1615924390842_1222 (state: ACCEPTED)
21/04/06 16:06:21 INFO Client: Application report for application_1615924390842_1222 (state: ACCEPTED)
21/04/06 16:06:22 INFO Client: Application report for application_1615924390842_1222 (state: ACCEPTED)
21/04/06 16:06:23 INFO Client: Application report for application_1615924390842_1222 (state: RUNNING)
21/04/06 16:06:23 INFO Client:
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: ip-96-113-30-77.nest.r53.xcal.tv
	 ApplicationMaster RPC port: 44335
	 queue: default
	 start time: 1617725178774
	 final status: UNDEFINED
	 tracking URL: http://ip-96-113-30-160.nest.r53.xcal.tv:20888/proxy/application_1615924390842_1222/
	 user: hadoop
21/04/06 16:06:24 INFO Client: Application report for application_1615924390842_1222 (state: RUNNING)
21/04/06 16:06:25 INFO Client: Application report for application_1615924390842_1222 (state: RUNNING)
21/04/06 16:06:26 INFO Client: Application report for application_1615924390842_1222 (state: RUNNING)
21/04/06 16:06:27 INFO Client: Application report for application_1615924390842_1222 (state: RUNNING)
21/04/06 16:06:28 INFO Client: Application report for application_1615924390842_1222 (state: RUNNING)
21/04/06 16:06:29 INFO Client: Application report for application_1615924390842_1222 (state: RUNNING)
21/04/06 16:06:30 INFO Client: Application report for application_1615924390842_1222 (state: RUNNING)
21/04/06 16:06:31 INFO Client: Application report for application_1615924390842_1222 (state: RUNNING)
21/04/06 16:06:32 INFO Client: Application report for application_1615924390842_1222 (state: RUNNING)
21/04/06 16:06:33 INFO Client: Application report for application_1615924390842_1222 (state: RUNNING)
21/04/06 16:06:34 INFO Client: Application report for application_1615924390842_1222 (state: RUNNING)
21/04/06 16:06:35 INFO Client: Application report for application_1615924390842_1222 (state: RUNNING)
21/04/06 16:06:36 INFO Client: Application report for application_1615924390842_1222 (state: RUNNING)
21/04/06 16:06:37 INFO Client: Application report for application_1615924390842_1222 (state: RUNNING)
21/04/06 16:06:38 INFO Client: Application report for application_1615924390842_1222 (state: RUNNING)
21/04/06 16:06:39 INFO Client: Application report for application_1615924390842_1222 (state: RUNNING)
21/04/06 16:06:40 INFO Client: Application report for application_1615924390842_1222 (state: RUNNING)
21/04/06 16:06:41 INFO Client: Application report for application_1615924390842_1222 (state: RUNNING)
21/04/06 16:06:42 INFO Client: Application report for application_1615924390842_1222 (state: RUNNING)
21/04/06 16:06:43 INFO Client: Application report for application_1615924390842_1222 (state: FINISHED)
21/04/06 16:06:43 INFO Client:
	 client token: N/A
	 diagnostics: User class threw exception: org.apache.spark.sql.streaming.StreamingQueryException: Could not obtain block: BP-1947480113-96.113.30.160-1615924351536:blk_1075315063_1786033 file=/streaming-jobs--checkpoints/offsets/95
=== Streaming Query ===
Identifier: pipelineSnmpSink [id = 7e48e689-1e71-417d-86ca-0a517fb6250a, runId = 0ab7c8bd-2bfa-4a2f-a22f-efddfe5ed301]
Current Committed Offsets: {}
Current Available Offsets: {}

Current State: ACTIVE
Thread State: RUNNABLE

Logical Plan:
WriteToMicroBatchDataSource org.apache.spark.sql.execution.streaming.sources.ForeachWriterTable$$anon$1$$anon$2@1ef1410e
+- SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).uuid, true, false) AS uuid#839, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).id, true, false) AS id#840, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).currValue AS currValue#841, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).time AS time#842L, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).updated AS updated#843, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).isDiff AS isDiff#844, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).diffValue AS diffValue#845, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).deviceName, true, false) AS deviceName#846, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).deviceIp, true, false) AS deviceIp#847, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifName, true, false) AS ifName#848, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifDesc, true, false) AS ifDesc#849, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).indicatorName, true, false) AS indicatorName#850, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).objectName, true, false) AS objectName#851, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).division, true, false) AS division#852, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).sourceType, true, false) AS sourceType#853, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).prevTime AS prevTime#854L, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).vendor, true, false) AS vendor#855, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).model, true, false) AS model#856, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborFqdn, true, false) AS neighborFqdn#857, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborIfName, true, false) AS neighborIfName#858, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborIp, true, false) AS neighborIp#859, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifIndex AS ifIndex#860, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).cpuType, true, false) AS cpuType#861, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).cpuName, true, false) AS cpuName#862, ... 12 more fields]
   +- MapElements yet.another.company.snmp.dataframe.SnmpDatasetUtils$$$Lambda$1638/1606922232@46ce5ea1, class yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, [StructField(uuid,StringType,true), StructField(id,StringType,true), StructField(currValue,DecimalType(38,18),true), StructField(time,LongType,false), StructField(updated,BooleanType,false), StructField(isDiff,BooleanType,false), StructField(diffValue,DoubleType,false), StructField(deviceName,StringType,true), StructField(deviceIp,StringType,true), StructField(ifName,StringType,true), StructField(ifDesc,StringType,true), StructField(indicatorName,StringType,true), StructField(objectName,StringType,true), StructField(division,StringType,true), StructField(sourceType,StringType,true), StructField(prevTime,LongType,false), StructField(vendor,StringType,true), StructField(model,StringType,true), StructField(neighborFqdn,StringType,true), StructField(neighborIfName,StringType,true), StructField(neighborIp,StringType,true), StructField(ifIndex,IntegerType,false), StructField(cpuType,StringType,true), StructField(cpuName,StringType,true), ... 12 more fields], obj#838: yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState
      +- DeserializeToObject newInstance(class yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState), obj#837: yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState
         +- SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).uuid, true, false) AS uuid#728, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).id, true, false) AS id#729, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).currValue AS currValue#730, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).time AS time#731L, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).updated AS updated#732, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).isDiff AS isDiff#733, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).diffValue AS diffValue#734, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).deviceName, true, false) AS deviceName#735, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).deviceIp, true, false) AS deviceIp#736, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifName, true, false) AS ifName#737, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifDesc, true, false) AS ifDesc#738, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).indicatorName, true, false) AS indicatorName#739, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).objectName, true, false) AS objectName#740, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).division, true, false) AS division#741, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).sourceType, true, false) AS sourceType#742, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).prevTime AS prevTime#743L, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).vendor, true, false) AS vendor#744, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).model, true, false) AS model#745, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborFqdn, true, false) AS neighborFqdn#746, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborIfName, true, false) AS neighborIfName#747, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborIp, true, false) AS neighborIp#748, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifIndex AS ifIndex#749, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).cpuType, true, false) AS cpuType#750, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).cpuName, true, false) AS cpuName#751, ... 12 more fields]
            +- MapElements yet.another.company.snmp.dataframe.SnmpDatasetUtils$$$Lambda$1635/1906092695@62c5a927, class yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, [StructField(uuid,StringType,true), StructField(id,StringType,true), StructField(currValue,DecimalType(38,18),true), StructField(time,LongType,false), StructField(updated,BooleanType,false), StructField(isDiff,BooleanType,false), StructField(diffValue,DoubleType,false), StructField(deviceName,StringType,true), StructField(deviceIp,StringType,true), StructField(ifName,StringType,true), StructField(ifDesc,StringType,true), StructField(indicatorName,StringType,true), StructField(objectName,StringType,true), StructField(division,StringType,true), StructField(sourceType,StringType,true), StructField(prevTime,LongType,false), StructField(vendor,StringType,true), StructField(model,StringType,true), StructField(neighborFqdn,StringType,true), StructField(neighborIfName,StringType,true), StructField(neighborIp,StringType,true), StructField(ifIndex,IntegerType,false), StructField(cpuType,StringType,true), StructField(cpuName,StringType,true), ... 12 more fields], obj#727: yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState
               +- DeserializeToObject newInstance(class yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState), obj#726: yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState
                  +- SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).uuid, true, false) AS uuid#617, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).id, true, false) AS id#618, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).currValue AS currValue#619, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).time AS time#620L, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).updated AS updated#621, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).isDiff AS isDiff#622, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).diffValue AS diffValue#623, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).deviceName, true, false) AS deviceName#624, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).deviceIp, true, false) AS deviceIp#625, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifName, true, false) AS ifName#626, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifDesc, true, false) AS ifDesc#627, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).indicatorName, true, false) AS indicatorName#628, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).objectName, true, false) AS objectName#629, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).division, true, false) AS division#630, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).sourceType, true, false) AS sourceType#631, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).prevTime AS prevTime#632L, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).vendor, true, false) AS vendor#633, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).model, true, false) AS model#634, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborFqdn, true, false) AS neighborFqdn#635, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborIfName, true, false) AS neighborIfName#636, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborIp, true, false) AS neighborIp#637, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifIndex AS ifIndex#638, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).cpuType, true, false) AS cpuType#639, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).cpuName, true, false) AS cpuName#640, ... 12 more fields]
                     +- MapElements yet.another.company.core.metrics.LongValStatsAccumulator$$Lambda$1633/1149752687@4ad754af, class yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, [StructField(uuid,StringType,true), StructField(id,StringType,true), StructField(currValue,DecimalType(38,18),true), StructField(time,LongType,false), StructField(updated,BooleanType,false), StructField(isDiff,BooleanType,false), StructField(diffValue,DoubleType,false), StructField(deviceName,StringType,true), StructField(deviceIp,StringType,true), StructField(ifName,StringType,true), StructField(ifDesc,StringType,true), StructField(indicatorName,StringType,true), StructField(objectName,StringType,true), StructField(division,StringType,true), StructField(sourceType,StringType,true), StructField(prevTime,LongType,false), StructField(vendor,StringType,true), StructField(model,StringType,true), StructField(neighborFqdn,StringType,true), StructField(neighborIfName,StringType,true), StructField(neighborIp,StringType,true), StructField(ifIndex,IntegerType,false), StructField(cpuType,StringType,true), StructField(cpuName,StringType,true), ... 12 more fields], obj#616: yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState
                        +- DeserializeToObject newInstance(class yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState), obj#615: yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState
                           +- TypedFilter yet.another.company.snmp.dataframe.SnmpDatasetUtils$$$Lambda$1630/164744846@5e6fcaa5, class yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, [StructField(uuid,StringType,true), StructField(id,StringType,true), StructField(currValue,DecimalType(38,18),true), StructField(time,LongType,false), StructField(updated,BooleanType,false), StructField(isDiff,BooleanType,false), StructField(diffValue,DoubleType,false), StructField(deviceName,StringType,true), StructField(deviceIp,StringType,true), StructField(ifName,StringType,true), StructField(ifDesc,StringType,true), StructField(indicatorName,StringType,true), StructField(objectName,StringType,true), StructField(division,StringType,true), StructField(sourceType,StringType,true), StructField(prevTime,LongType,false), StructField(vendor,StringType,true), StructField(model,StringType,true), StructField(neighborFqdn,StringType,true), StructField(neighborIfName,StringType,true), StructField(neighborIp,StringType,true), StructField(ifIndex,IntegerType,false), StructField(cpuType,StringType,true), StructField(cpuName,StringType,true), ... 12 more fields], newInstance(class yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState)
                              +- SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).uuid, true, false) AS uuid#469, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).id, true, false) AS id#470, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).currValue AS currValue#471, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).time AS time#472L, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).updated AS updated#473, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).isDiff AS isDiff#474, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).diffValue AS diffValue#475, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).deviceName, true, false) AS deviceName#476, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).deviceIp, true, false) AS deviceIp#477, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifName, true, false) AS ifName#478, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifDesc, true, false) AS ifDesc#479, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).indicatorName, true, false) AS indicatorName#480, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).objectName, true, false) AS objectName#481, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).division, true, false) AS division#482, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).sourceType, true, false) AS sourceType#483, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).prevTime AS prevTime#484L, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).vendor, true, false) AS vendor#485, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).model, true, false) AS model#486, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborFqdn, true, false) AS neighborFqdn#487, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborIfName, true, false) AS neighborIfName#488, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborIp, true, false) AS neighborIp#489, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifIndex AS ifIndex#490, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).cpuType, true, false) AS cpuType#491, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).cpuName, true, false) AS cpuName#492, ... 12 more fields]
                                 +- MapElements yet.another.company.snmp.dataframe.SnmpDatasetUtils$$$Lambda$1629/2141847925@6a60ac9f, class yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, [StructField(uuid,StringType,true), StructField(id,StringType,true), StructField(currValue,DecimalType(38,18),true), StructField(time,LongType,false), StructField(updated,BooleanType,false), StructField(isDiff,BooleanType,false), StructField(diffValue,DoubleType,false), StructField(deviceName,StringType,true), StructField(deviceIp,StringType,true), StructField(ifName,StringType,true), StructField(ifDesc,StringType,true), StructField(indicatorName,StringType,true), StructField(objectName,StringType,true), StructField(division,StringType,true), StructField(sourceType,StringType,true), StructField(prevTime,LongType,false), StructField(vendor,StringType,true), StructField(model,StringType,true), StructField(neighborFqdn,StringType,true), StructField(neighborIfName,StringType,true), StructField(neighborIp,StringType,true), StructField(ifIndex,IntegerType,false), StructField(cpuType,StringType,true), StructField(cpuName,StringType,true), ... 12 more fields], obj#468: yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState
                                    +- DeserializeToObject newInstance(class yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState), obj#467: yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState
                                       +- SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).uuid, true, false) AS uuid#358, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).id, true, false) AS id#359, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).currValue AS currValue#360, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).time AS time#361L, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).updated AS updated#362, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).isDiff AS isDiff#363, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).diffValue AS diffValue#364, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).deviceName, true, false) AS deviceName#365, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).deviceIp, true, false) AS deviceIp#366, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifName, true, false) AS ifName#367, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifDesc, true, false) AS ifDesc#368, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).indicatorName, true, false) AS indicatorName#369, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).objectName, true, false) AS objectName#370, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).division, true, false) AS division#371, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).sourceType, true, false) AS sourceType#372, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).prevTime AS prevTime#373L, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).vendor, true, false) AS vendor#374, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).model, true, false) AS model#375, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborFqdn, true, false) AS neighborFqdn#376, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborIfName, true, false) AS neighborIfName#377, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborIp, true, false) AS neighborIp#378, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifIndex AS ifIndex#379, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).cpuType, true, false) AS cpuType#380, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).cpuName, true, false) AS cpuName#381, ... 12 more fields]
                                          +- FlatMapGroupsWithState yet.another.company.snmp.dataframe.SnmpDatasetUtils$$$Lambda$1621/1596697168@2523bc65, cast(value#284 as string).toString, newInstance(class yet.another.company.snmp.dto.RichSnmpRecord), [value#284], [deviceId#254, deviceName#255, deviceIp#256, objectName#257, ifDesc#258, indicatorName#259, value#260, time#261L, pluginName#262, vendor#263, model#264, neighborFqdn#265, neighborIfName#266, neighborIp#267, ifIndex#268, id#269, sourceType#270, cpuType#271, cpuName#272, ifName#273, division#274, metric#275, status#276, bandwidth#277L, ... 5 more fields], obj#357: yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, class[uuid[0]: string, id[0]: string, currValue[0]: decimal(38,18), time[0]: bigint, updated[0]: boolean, isDiff[0]: boolean, diffValue[0]: double, deviceName[0]: string, deviceIp[0]: string, ifName[0]: string, ifDesc[0]: string, indicatorName[0]: string, objectName[0]: string, division[0]: string, sourceType[0]: string, prevTime[0]: bigint, vendor[0]: string, model[0]: string, neighborFqdn[0]: string, neighborIfName[0]: string, neighborIp[0]: string, ifIndex[0]: int, cpuType[0]: string, cpuName[0]: string, status[0]: struct<underMaintenance:boolean,inInventory:boolean,neighborUnderMaintenance:boolean>, metric[0]: string, tags[0]: map<string,string>, sysId[0]: string, rate[0]: double, bandwidth[0]: bigint, connectionId[0]: string, history[0]: array<struct<time:bigint,value:decimal(38,18)>>, metaInfo[0]: struct<batchId:bigint>, historyRecords[0]: array<struct<ruleId:string,ruleDescription:string,workflowId:string,className:string,nodeIp:string,eventTime:string>>, forAlarms[0]: boolean, eventLevel[0]: string], Update, false, NoTimeout
                                             +- AppendColumns yet.another.company.snmp.dataframe.SnmpDatasetUtils$$$Lambda$1619/1199596405@55a1f642, class yet.another.company.snmp.dto.RichSnmpRecord, [StructField(deviceId,IntegerType,false), StructField(deviceName,StringType,true), StructField(deviceIp,StringType,true), StructField(objectName,StringType,true), StructField(ifDesc,StringType,true), StructField(indicatorName,StringType,true), StructField(value,DecimalType(38,18),true), StructField(time,LongType,false), StructField(pluginName,StringType,true), StructField(vendor,StringType,true), StructField(model,StringType,true), StructField(neighborFqdn,StringType,true), StructField(neighborIfName,StringType,true), StructField(neighborIp,StringType,true), StructField(ifIndex,IntegerType,false), StructField(id,StringType,true), StructField(sourceType,StringType,true), StructField(cpuType,StringType,true), StructField(cpuName,StringType,true), StructField(ifName,StringType,true), StructField(division,StringType,true), StructField(metric,StringType,true), StructField(status,StructType(StructField(underMaintenance,BooleanType,false), StructField(inInventory,BooleanType,false), StructField(neighborUnderMaintenance,BooleanType,true)),true), StructField(bandwidth,LongType,false), ... 5 more fields], newInstance(class yet.another.company.snmp.dto.RichSnmpRecord), [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, java.lang.String, true], true, false) AS value#284]
                                                +- SerializeFromObject [knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).deviceId AS deviceId#254, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).deviceName, true, false) AS deviceName#255, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).deviceIp, true, false) AS deviceIp#256, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).objectName, true, false) AS objectName#257, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).ifDesc, true, false) AS ifDesc#258, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).indicatorName, true, false) AS indicatorName#259, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).value AS value#260, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).time AS time#261L, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).pluginName, true, false) AS pluginName#262, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).vendor, true, false) AS vendor#263, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).model, true, false) AS model#264, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).neighborFqdn, true, false) AS neighborFqdn#265, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).neighborIfName, true, false) AS neighborIfName#266, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).neighborIp, true, false) AS neighborIp#267, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).ifIndex AS ifIndex#268, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).id, true, false) AS id#269, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).sourceType, true, false) AS sourceType#270, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).cpuType, true, false) AS cpuType#271, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).cpuName, true, false) AS cpuName#272, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).ifName, true, false) AS ifName#273, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).division, true, false) AS division#274, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).metric, true, false) AS metric#275, if (isnull(knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).status)) null else named_struct(underMaintenance, knownnotnull(knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).status).underMaintenance, inInventory, knownnotnull(knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).status).inInventory, neighborUnderMaintenance, unwrapoption(BooleanType, knownnotnull(knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).status).neighborUnderMaintenance)) AS status#276, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).bandwidth AS bandwidth#277L, ... 5 more fields]
                                                   +- MapPartitions org.apache.spark.sql.Dataset$$Lambda$1407/1797617004@1ad07acd, obj#253: yet.another.company.snmp.dto.RichSnmpRecord
                                                      +- DeserializeToObject newInstance(class yet.another.company.snmp.dto.RichSnmpRecord), obj#252: yet.another.company.snmp.dto.RichSnmpRecord
                                                         +- SerializeFromObject [knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).deviceId AS deviceId#194, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).deviceName, true, false) AS deviceName#195, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).deviceIp, true, false) AS deviceIp#196, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).objectName, true, false) AS objectName#197, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).ifDesc, true, false) AS ifDesc#198, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).indicatorName, true, false) AS indicatorName#199, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).value AS value#200, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).time AS time#201L, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).pluginName, true, false) AS pluginName#202, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).vendor, true, false) AS vendor#203, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).model, true, false) AS model#204, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).neighborFqdn, true, false) AS neighborFqdn#205, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).neighborIfName, true, false) AS neighborIfName#206, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).neighborIp, true, false) AS neighborIp#207, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).ifIndex AS ifIndex#208, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).id, true, false) AS id#209, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).sourceType, true, false) AS sourceType#210, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).cpuType, true, false) AS cpuType#211, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).cpuName, true, false) AS cpuName#212, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).ifName, true, false) AS ifName#213, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).division, true, false) AS division#214, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).metric, true, false) AS metric#215, if (isnull(knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).status)) null else named_struct(underMaintenance, knownnotnull(knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).status).underMaintenance, inInventory, knownnotnull(knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).status).inInventory, neighborUnderMaintenance, unwrapoption(BooleanType, knownnotnull(knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).status).neighborUnderMaintenance)) AS status#216, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).bandwidth AS bandwidth#217L, ... 5 more fields]
                                                            +- MapPartitions org.apache.spark.sql.Dataset$$Lambda$1407/1797617004@22a82d66, obj#193: yet.another.company.snmp.dto.RichSnmpRecord
                                                               +- DeserializeToObject newInstance(class yet.another.company.snmp.streaming.InputSevOneRecord), obj#192: yet.another.company.snmp.streaming.InputSevOneRecord
                                                                  +- Union
                                                                     :- TypedFilter yet.another.company.snmp.streaming.SnmpStreaming$$$Lambda$1425/1154254244@78e1cc97, class yet.another.company.snmp.streaming.InputSevOneRecord, [StructField(deviceId,IntegerType,false), StructField(deviceName,StringType,true), StructField(deviceIp,StringType,true), StructField(peerId,IntegerType,false), StructField(objectId,IntegerType,false), StructField(objectName,StringType,true), StructField(objectDesc,StringType,true), StructField(pluginId,IntegerType,false), StructField(pluginName,StringType,true), StructField(indicatorId,IntegerType,false), StructField(indicatorName,StringType,true), StructField(format,IntegerType,false), StructField(value,StringType,true), StructField(time,DoubleType,false), StructField(clusterName,StringType,true), StructField(peerIp,StringType,true)], newInstance(class yet.another.company.snmp.streaming.InputSevOneRecord)
                                                                     :  +- SerializeFromObject [knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).deviceId AS deviceId#40, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).deviceName, true, false) AS deviceName#41, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).deviceIp, true, false) AS deviceIp#42, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).peerId AS peerId#43, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).objectId AS objectId#44, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).objectName, true, false) AS objectName#45, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).objectDesc, true, false) AS objectDesc#46, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).pluginId AS pluginId#47, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).pluginName, true, false) AS pluginName#48, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).indicatorId AS indicatorId#49, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).indicatorName, true, false) AS indicatorName#50, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).format AS format#51, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).value, true, false) AS value#52, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).time AS time#53, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).clusterName, true, false) AS clusterName#54, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).peerIp, true, false) AS peerIp#55]
                                                                     :     +- MapPartitions org.apache.spark.sql.Dataset$$Lambda$1407/1797617004@54d3e196, obj#39: yet.another.company.snmp.streaming.InputSevOneRecord
                                                                     :        +- DeserializeToObject cast(value#8 as binary), obj#38: binary
                                                                     :           +- Project [value#8]
                                                                     :              +- StreamingDataSourceV2Relation [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan@4d4cffd3, KafkaV2[Subscribe[sevone-spdb]]
                                                                     +- TypedFilter yet.another.company.snmp.streaming.SnmpStreaming$$$Lambda$1425/1154254244@78e1cc97, class yet.another.company.snmp.streaming.InputSevOneRecord, [StructField(deviceId,IntegerType,false), StructField(deviceName,StringType,true), StructField(deviceIp,StringType,true), StructField(peerId,IntegerType,false), StructField(objectId,IntegerType,false), StructField(objectName,StringType,true), StructField(objectDesc,StringType,true), StructField(pluginId,IntegerType,false), StructField(pluginName,StringType,true), StructField(indicatorId,IntegerType,false), StructField(indicatorName,StringType,true), StructField(format,IntegerType,false), StructField(value,StringType,true), StructField(time,DoubleType,false), StructField(clusterName,StringType,true), StructField(peerIp,StringType,true)], newInstance(class yet.another.company.snmp.streaming.InputSevOneRecord)
                                                                        +- SerializeFromObject [knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).deviceId AS deviceId#113, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).deviceName, true, false) AS deviceName#114, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).deviceIp, true, false) AS deviceIp#115, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).peerId AS peerId#116, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).objectId AS objectId#117, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).objectName, true, false) AS objectName#118, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).objectDesc, true, false) AS objectDesc#119, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).pluginId AS pluginId#120, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).pluginName, true, false) AS pluginName#121, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).indicatorId AS indicatorId#122, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).indicatorName, true, false) AS indicatorName#123, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).format AS format#124, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).value, true, false) AS value#125, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).time AS time#126, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).clusterName, true, false) AS clusterName#127, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).peerIp, true, false) AS peerIp#128]
                                                                           +- MapPartitions org.apache.spark.sql.Dataset$$Lambda$1407/1797617004@2e6c3801, obj#112: yet.another.company.snmp.streaming.InputSevOneRecord
                                                                              +- DeserializeToObject cast(value#81 as binary), obj#111: binary
                                                                                 +- Project [value#81]
                                                                                    +- StreamingDataSourceV2Relation [key#80, value#81, topic#82, partition#83, offset#84L, timestamp#85, timestampType#86], org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan@581fcc2f, KafkaV2[Subscribe[sevone-spdb]]

	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:355)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:245)
Caused by: org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-1947480113-96.113.30.160-1615924351536:blk_1075315063_1786033 file=/streaming-jobs--checkpoints/offsets/95
	at org.apache.hadoop.hdfs.DFSInputStream.refetchLocations(DFSInputStream.java:879)
	at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:862)
	at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:841)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:567)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:757)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:829)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	at java.io.InputStreamReader.read(InputStreamReader.java:184)
	at java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.io.BufferedReader.readLine(BufferedReader.java:324)
	at java.io.BufferedReader.readLine(BufferedReader.java:389)
	at scala.io.BufferedSource$BufferedLineIterator.hasNext(BufferedSource.scala:74)
	at org.apache.spark.sql.execution.streaming.OffsetSeqLog.deserialize(OffsetSeqLog.scala:56)
	at org.apache.spark.sql.execution.streaming.OffsetSeqLog.deserialize(OffsetSeqLog.scala:46)
	at org.apache.spark.sql.execution.streaming.HDFSMetadataLog.get(HDFSMetadataLog.scala:153)
	at org.apache.spark.sql.execution.streaming.HDFSMetadataLog.$anonfun$getLatest$2(HDFSMetadataLog.scala:190)
	at scala.runtime.java8.JFunction1$mcVJ$sp.apply(JFunction1$mcVJ$sp.java:23)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofLong.foreach(ArrayOps.scala:258)
	at org.apache.spark.sql.execution.streaming.HDFSMetadataLog.getLatest(HDFSMetadataLog.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.populateStartOffsets(MicroBatchExecution.scala:272)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:194)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:352)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:350)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:191)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:185)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:334)
	... 1 more

	 ApplicationMaster host: ip-96-113-30-77.nest.r53.xcal.tv
	 ApplicationMaster RPC port: 44335
	 queue: default
	 start time: 1617725178774
	 final status: FAILED
	 tracking URL: http://ip-96-113-30-160.nest.r53.xcal.tv:20888/proxy/application_1615924390842_1222/
	 user: hadoop
21/04/06 16:06:43 ERROR Client: Application diagnostics message: User class threw exception: org.apache.spark.sql.streaming.StreamingQueryException: Could not obtain block: BP-1947480113-96.113.30.160-1615924351536:blk_1075315063_1786033 file=/streaming-jobs--checkpoints/offsets/95
=== Streaming Query ===
Identifier: pipelineSnmpSink [id = 7e48e689-1e71-417d-86ca-0a517fb6250a, runId = 0ab7c8bd-2bfa-4a2f-a22f-efddfe5ed301]
Current Committed Offsets: {}
Current Available Offsets: {}

Current State: ACTIVE
Thread State: RUNNABLE

Logical Plan:
WriteToMicroBatchDataSource org.apache.spark.sql.execution.streaming.sources.ForeachWriterTable$$anon$1$$anon$2@1ef1410e
+- SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).uuid, true, false) AS uuid#839, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).id, true, false) AS id#840, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).currValue AS currValue#841, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).time AS time#842L, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).updated AS updated#843, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).isDiff AS isDiff#844, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).diffValue AS diffValue#845, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).deviceName, true, false) AS deviceName#846, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).deviceIp, true, false) AS deviceIp#847, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifName, true, false) AS ifName#848, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifDesc, true, false) AS ifDesc#849, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).indicatorName, true, false) AS indicatorName#850, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).objectName, true, false) AS objectName#851, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).division, true, false) AS division#852, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).sourceType, true, false) AS sourceType#853, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).prevTime AS prevTime#854L, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).vendor, true, false) AS vendor#855, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).model, true, false) AS model#856, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborFqdn, true, false) AS neighborFqdn#857, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborIfName, true, false) AS neighborIfName#858, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborIp, true, false) AS neighborIp#859, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifIndex AS ifIndex#860, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).cpuType, true, false) AS cpuType#861, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).cpuName, true, false) AS cpuName#862, ... 12 more fields]
   +- MapElements yet.another.company.snmp.dataframe.SnmpDatasetUtils$$$Lambda$1638/1606922232@46ce5ea1, class yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, [StructField(uuid,StringType,true), StructField(id,StringType,true), StructField(currValue,DecimalType(38,18),true), StructField(time,LongType,false), StructField(updated,BooleanType,false), StructField(isDiff,BooleanType,false), StructField(diffValue,DoubleType,false), StructField(deviceName,StringType,true), StructField(deviceIp,StringType,true), StructField(ifName,StringType,true), StructField(ifDesc,StringType,true), StructField(indicatorName,StringType,true), StructField(objectName,StringType,true), StructField(division,StringType,true), StructField(sourceType,StringType,true), StructField(prevTime,LongType,false), StructField(vendor,StringType,true), StructField(model,StringType,true), StructField(neighborFqdn,StringType,true), StructField(neighborIfName,StringType,true), StructField(neighborIp,StringType,true), StructField(ifIndex,IntegerType,false), StructField(cpuType,StringType,true), StructField(cpuName,StringType,true), ... 12 more fields], obj#838: yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState
      +- DeserializeToObject newInstance(class yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState), obj#837: yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState
         +- SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).uuid, true, false) AS uuid#728, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).id, true, false) AS id#729, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).currValue AS currValue#730, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).time AS time#731L, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).updated AS updated#732, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).isDiff AS isDiff#733, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).diffValue AS diffValue#734, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).deviceName, true, false) AS deviceName#735, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).deviceIp, true, false) AS deviceIp#736, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifName, true, false) AS ifName#737, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifDesc, true, false) AS ifDesc#738, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).indicatorName, true, false) AS indicatorName#739, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).objectName, true, false) AS objectName#740, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).division, true, false) AS division#741, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).sourceType, true, false) AS sourceType#742, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).prevTime AS prevTime#743L, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).vendor, true, false) AS vendor#744, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).model, true, false) AS model#745, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborFqdn, true, false) AS neighborFqdn#746, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborIfName, true, false) AS neighborIfName#747, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborIp, true, false) AS neighborIp#748, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifIndex AS ifIndex#749, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).cpuType, true, false) AS cpuType#750, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).cpuName, true, false) AS cpuName#751, ... 12 more fields]
            +- MapElements yet.another.company.snmp.dataframe.SnmpDatasetUtils$$$Lambda$1635/1906092695@62c5a927, class yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, [StructField(uuid,StringType,true), StructField(id,StringType,true), StructField(currValue,DecimalType(38,18),true), StructField(time,LongType,false), StructField(updated,BooleanType,false), StructField(isDiff,BooleanType,false), StructField(diffValue,DoubleType,false), StructField(deviceName,StringType,true), StructField(deviceIp,StringType,true), StructField(ifName,StringType,true), StructField(ifDesc,StringType,true), StructField(indicatorName,StringType,true), StructField(objectName,StringType,true), StructField(division,StringType,true), StructField(sourceType,StringType,true), StructField(prevTime,LongType,false), StructField(vendor,StringType,true), StructField(model,StringType,true), StructField(neighborFqdn,StringType,true), StructField(neighborIfName,StringType,true), StructField(neighborIp,StringType,true), StructField(ifIndex,IntegerType,false), StructField(cpuType,StringType,true), StructField(cpuName,StringType,true), ... 12 more fields], obj#727: yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState
               +- DeserializeToObject newInstance(class yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState), obj#726: yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState
                  +- SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).uuid, true, false) AS uuid#617, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).id, true, false) AS id#618, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).currValue AS currValue#619, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).time AS time#620L, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).updated AS updated#621, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).isDiff AS isDiff#622, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).diffValue AS diffValue#623, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).deviceName, true, false) AS deviceName#624, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).deviceIp, true, false) AS deviceIp#625, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifName, true, false) AS ifName#626, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifDesc, true, false) AS ifDesc#627, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).indicatorName, true, false) AS indicatorName#628, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).objectName, true, false) AS objectName#629, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).division, true, false) AS division#630, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).sourceType, true, false) AS sourceType#631, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).prevTime AS prevTime#632L, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).vendor, true, false) AS vendor#633, staticinvoke(class org.apache.sCommand exiting with ret '1'
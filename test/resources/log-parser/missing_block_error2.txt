Ivy Default Cache set to: /home/hadoop/.ivy2/cache
The jars for the packages stored in: /home/hadoop/.ivy2/jars
:: loading settings :: url = jar:file:/usr/lib/spark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
com.qubole.spark#spark-sql-kinesis_2.12 added as a dependency
mysql#mysql-connector-java added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-eafbfa69-ffc9-41d2-9a5c-8aab972c7d27;1.0
	confs: [default]
	found org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 in central
	found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 in central
	found org.apache.kafka#kafka-clients;2.4.1 in central
	found com.github.luben#zstd-jni;1.4.4-3 in central
	found org.lz4#lz4-java;1.7.1 in central
	found org.xerial.snappy#snappy-java;1.1.7.5 in central
	found org.slf4j#slf4j-api;1.7.30 in central
	found org.spark-project.spark#unused;1.0.0 in central
	found org.apache.commons#commons-pool2;2.6.2 in central
	found com.qubole.spark#spark-sql-kinesis_2.12;1.2.0_spark-3.0 in central
	found mysql#mysql-connector-java;8.0.20 in central
	found com.google.protobuf#protobuf-java;3.6.1 in central
:: resolution report :: resolve 263ms :: artifacts dl 6ms
	:: modules in use:
	com.github.luben#zstd-jni;1.4.4-3 from central in [default]
	com.google.protobuf#protobuf-java;3.6.1 from central in [default]
	com.qubole.spark#spark-sql-kinesis_2.12;1.2.0_spark-3.0 from central in [default]
	mysql#mysql-connector-java;8.0.20 from central in [default]
	org.apache.commons#commons-pool2;2.6.2 from central in [default]
	org.apache.kafka#kafka-clients;2.4.1 from central in [default]
	org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 from central in [default]
	org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 from central in [default]
	org.lz4#lz4-java;1.7.1 from central in [default]
	org.slf4j#slf4j-api;1.7.30 from central in [default]
	org.spark-project.spark#unused;1.0.0 from central in [default]
	org.xerial.snappy#snappy-java;1.1.7.5 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-eafbfa69-ffc9-41d2-9a5c-8aab972c7d27
	confs: [default]
	0 artifacts copied, 12 already retrieved (0kB/7ms)
20/12/07 11:33:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/07 11:33:24 INFO RMProxy: Connecting to ResourceManager at ip-96-113-30-135.nest.r53.xcal.tv/96.113.30.135:8032
20/12/07 11:33:24 INFO Client: Requesting a new application from cluster with 18 NodeManagers
20/12/07 11:33:25 INFO Configuration: resource-types.xml not found
20/12/07 11:33:25 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/12/07 11:33:25 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (57344 MB per container)
20/12/07 11:33:25 INFO Client: Will allocate AM container, with 11264 MB memory including 1024 MB overhead
20/12/07 11:33:25 INFO Client: Setting up container launch context for our AM
20/12/07 11:33:25 INFO Client: Setting up the launch environment for our AM container
20/12/07 11:33:25 INFO Client: Preparing resources for our AM container
20/12/07 11:33:25 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/07 11:33:27 INFO Client: Uploading resource file:/mnt/tmp/spark-3758d98d-0589-4668-8f9a-917923cb762d/__spark_libs__724227489021463416.zip -> hdfs://ip-96-113-30-135.nest.r53.xcal.tv:8020/user/hadoop/.sparkStaging/application_1607028964151_1511/__spark_libs__724227489021463416.zip
20/12/07 11:33:27 INFO Client: Uploading resource file:/home/hadoop/streaming-jobs-.snmp-assembly-1.4-SNAPSHOT.jar -> hdfs://ip-96-113-30-135.nest.r53.xcal.tv:8020/user/hadoop/.sparkStaging/application_1607028964151_1511/streaming-jobs-.snmp-assembly-1.4-SNAPSHOT.jar
20/12/07 11:33:27 INFO Client: Uploading resource file:/home/hadoop/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.0.0.jar -> hdfs://ip-96-113-30-135.nest.r53.xcal.tv:8020/user/hadoop/.sparkStaging/application_1607028964151_1511/org.apache.spark_spark-sql-kafka-0-10_2.12-3.0.0.jar
20/12/07 11:33:27 INFO Client: Uploading resource file:/home/hadoop/.ivy2/jars/com.qubole.spark_spark-sql-kinesis_2.12-1.2.0_spark-3.0.jar -> hdfs://ip-96-113-30-135.nest.r53.xcal.tv:8020/user/hadoop/.sparkStaging/application_1607028964151_1511/com.qubole.spark_spark-sql-kinesis_2.12-1.2.0_spark-3.0.jar
20/12/07 11:33:27 INFO Client: Uploading resource file:/home/hadoop/.ivy2/jars/mysql_mysql-connector-java-8.0.20.jar -> hdfs://ip-96-113-30-135.nest.r53.xcal.tv:8020/user/hadoop/.sparkStaging/application_1607028964151_1511/mysql_mysql-connector-java-8.0.20.jar
20/12/07 11:33:27 INFO Client: Uploading resource file:/home/hadoop/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.0.0.jar -> hdfs://ip-96-113-30-135.nest.r53.xcal.tv:8020/user/hadoop/.sparkStaging/application_1607028964151_1511/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.0.0.jar
20/12/07 11:33:27 INFO Client: Uploading resource file:/home/hadoop/.ivy2/jars/org.apache.kafka_kafka-clients-2.4.1.jar -> hdfs://ip-96-113-30-135.nest.r53.xcal.tv:8020/user/hadoop/.sparkStaging/application_1607028964151_1511/org.apache.kafka_kafka-clients-2.4.1.jar
20/12/07 11:33:27 INFO Client: Uploading resource file:/home/hadoop/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar -> hdfs://ip-96-113-30-135.nest.r53.xcal.tv:8020/user/hadoop/.sparkStaging/application_1607028964151_1511/org.apache.commons_commons-pool2-2.6.2.jar
20/12/07 11:33:27 INFO Client: Uploading resource file:/home/hadoop/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar -> hdfs://ip-96-113-30-135.nest.r53.xcal.tv:8020/user/hadoop/.sparkStaging/application_1607028964151_1511/org.spark-project.spark_unused-1.0.0.jar
20/12/07 11:33:27 INFO Client: Uploading resource file:/home/hadoop/.ivy2/jars/com.github.luben_zstd-jni-1.4.4-3.jar -> hdfs://ip-96-113-30-135.nest.r53.xcal.tv:8020/user/hadoop/.sparkStaging/application_1607028964151_1511/com.github.luben_zstd-jni-1.4.4-3.jar
20/12/07 11:33:27 INFO Client: Uploading resource file:/home/hadoop/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar -> hdfs://ip-96-113-30-135.nest.r53.xcal.tv:8020/user/hadoop/.sparkStaging/application_1607028964151_1511/org.lz4_lz4-java-1.7.1.jar
20/12/07 11:33:27 INFO Client: Uploading resource file:/home/hadoop/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar -> hdfs://ip-96-113-30-135.nest.r53.xcal.tv:8020/user/hadoop/.sparkStaging/application_1607028964151_1511/org.xerial.snappy_snappy-java-1.1.7.5.jar
20/12/07 11:33:27 INFO Client: Uploading resource file:/home/hadoop/.ivy2/jars/org.slf4j_slf4j-api-1.7.30.jar -> hdfs://ip-96-113-30-135.nest.r53.xcal.tv:8020/user/hadoop/.sparkStaging/application_1607028964151_1511/org.slf4j_slf4j-api-1.7.30.jar
20/12/07 11:33:27 INFO Client: Uploading resource file:/home/hadoop/.ivy2/jars/com.google.protobuf_protobuf-java-3.6.1.jar -> hdfs://ip-96-113-30-135.nest.r53.xcal.tv:8020/user/hadoop/.sparkStaging/application_1607028964151_1511/com.google.protobuf_protobuf-java-3.6.1.jar
20/12/07 11:33:27 INFO Client: Uploading resource file:/mnt/tmp/spark-3758d98d-0589-4668-8f9a-917923cb762d/__spark_conf__9088257520851637775.zip -> hdfs://ip-96-113-30-135.nest.r53.xcal.tv:8020/user/hadoop/.sparkStaging/application_1607028964151_1511/__spark_conf__.zip
20/12/07 11:33:27 INFO SecurityManager: Changing view acls to: hadoop
20/12/07 11:33:27 INFO SecurityManager: Changing modify acls to: hadoop
20/12/07 11:33:27 INFO SecurityManager: Changing view acls groups to: 
20/12/07 11:33:27 INFO SecurityManager: Changing modify acls groups to: 
20/12/07 11:33:27 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hadoop); groups with view permissions: Set(); users  with modify permissions: Set(hadoop); groups with modify permissions: Set()
20/12/07 11:33:28 INFO Client: Submitting application application_1607028964151_1511 to ResourceManager
20/12/07 11:33:28 INFO YarnClientImpl: Submitted application application_1607028964151_1511
20/12/07 11:33:29 INFO Client: Application report for application_1607028964151_1511 (state: ACCEPTED)
20/12/07 11:33:29 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1607340808025
	 final status: UNDEFINED
	 tracking URL: http://ip-96-113-30-135.nest.r53.xcal.tv:20888/proxy/application_1607028964151_1511/
	 user: hadoop
20/12/07 11:33:30 INFO Client: Application report for application_1607028964151_1511 (state: ACCEPTED)
20/12/07 11:33:31 INFO Client: Application report for application_1607028964151_1511 (state: ACCEPTED)
20/12/07 11:33:32 INFO Client: Application report for application_1607028964151_1511 (state: ACCEPTED)
20/12/07 11:33:33 INFO Client: Application report for application_1607028964151_1511 (state: RUNNING)
20/12/07 11:33:33 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: ip-96-113-30-118.nest.r53.xcal.tv
	 ApplicationMaster RPC port: 33667
	 queue: default
	 start time: 1607340808025
	 final status: UNDEFINED
	 tracking URL: http://ip-96-113-30-135.nest.r53.xcal.tv:20888/proxy/application_1607028964151_1511/
	 user: hadoop
20/12/07 11:33:34 INFO Client: Application report for application_1607028964151_1511 (state: RUNNING)
20/12/07 11:33:35 INFO Client: Application report for application_1607028964151_1511 (state: RUNNING)
20/12/07 11:33:36 INFO Client: Application report for application_1607028964151_1511 (state: RUNNING)
20/12/07 11:33:37 INFO Client: Application report for application_1607028964151_1511 (state: RUNNING)
20/12/07 11:33:38 INFO Client: Application report for application_1607028964151_1511 (state: RUNNING)
20/12/07 11:33:39 INFO Client: Application report for application_1607028964151_1511 (state: RUNNING)
20/12/07 11:33:40 INFO Client: Application report for application_1607028964151_1511 (state: RUNNING)
20/12/07 11:33:41 INFO Client: Application report for application_1607028964151_1511 (state: RUNNING)
20/12/07 11:33:42 INFO Client: Application report for application_1607028964151_1511 (state: RUNNING)
20/12/07 11:33:43 INFO Client: Application report for application_1607028964151_1511 (state: RUNNING)
20/12/07 11:33:44 INFO Client: Application report for application_1607028964151_1511 (state: RUNNING)
20/12/07 11:33:45 INFO Client: Application report for application_1607028964151_1511 (state: RUNNING)
20/12/07 11:33:46 INFO Client: Application report for application_1607028964151_1511 (state: RUNNING)
20/12/07 11:33:47 INFO Client: Application report for application_1607028964151_1511 (state: RUNNING)
20/12/07 11:33:48 INFO Client: Application report for application_1607028964151_1511 (state: RUNNING)
20/12/07 11:33:49 INFO Client: Application report for application_1607028964151_1511 (state: RUNNING)
20/12/07 11:33:50 INFO Client: Application report for application_1607028964151_1511 (state: RUNNING)
20/12/07 11:33:51 INFO Client: Application report for application_1607028964151_1511 (state: RUNNING)
20/12/07 11:33:52 INFO Client: Application report for application_1607028964151_1511 (state: RUNNING)
20/12/07 11:33:53 INFO Client: Application report for application_1607028964151_1511 (state: RUNNING)
20/12/07 11:33:54 INFO Client: Application report for application_1607028964151_1511 (state: RUNNING)
20/12/07 11:33:55 INFO Client: Application report for application_1607028964151_1511 (state: RUNNING)
20/12/07 11:33:56 INFO Client: Application report for application_1607028964151_1511 (state: FINISHED)
20/12/07 11:33:56 INFO Client: 
	 client token: N/A
	 diagnostics: Diagnostic messages truncated, showing last 65536 chars out of 75821:
...e])).indicatorName, true, false) AS indicatorName#774, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).objectName, true, false) AS objectName#775, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).division, true, false) AS division#776, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).sourceType, true, false) AS sourceType#777, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).prevTime AS prevTime#778L, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).vendor, true, false) AS vendor#779, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).model, true, false) AS model#780, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborFqdn, true, false) AS neighborFqdn#781, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborIfName, true, false) AS neighborIfName#782, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborIp, true, false) AS neighborIp#783, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifIndex AS ifIndex#784, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).cpuType, true, false) AS cpuType#785, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).cpuName, true, false) AS cpuName#786, ... 8 more fields]
               +- MapElements yet.another.company.snmp.dataframe.SnmpDatasetUtils$$$Lambda$1609/1385079241@411e703b, class yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, [StructField(uuid,StringType,true), StructField(id,StringType,true), StructField(currValue,DecimalType(38,18),true), StructField(time,LongType,false), StructField(updated,BooleanType,false), StructField(isDiff,BooleanType,false), StructField(diffValue,DoubleType,false), StructField(deviceName,StringType,true), StructField(deviceIp,StringType,true), StructField(ifName,StringType,true), StructField(ifDesc,StringType,true), StructField(indicatorName,StringType,true), StructField(objectName,StringType,true), StructField(division,StringType,true), StructField(sourceType,StringType,true), StructField(prevTime,LongType,false), StructField(vendor,StringType,true), StructField(model,StringType,true), StructField(neighborFqdn,StringType,true), StructField(neighborIfName,StringType,true), StructField(neighborIp,StringType,true), StructField(ifIndex,IntegerType,false), StructField(cpuType,StringType,true), StructField(cpuName,StringType,true), ... 8 more fields], obj#762: yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState
                  +- DeserializeToObject newInstance(class yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState), obj#761: yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState
                     +- SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).uuid, true, false) AS uuid#664, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).id, true, false) AS id#665, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).currValue AS currValue#666, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).time AS time#667L, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).updated AS updated#668, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).isDiff AS isDiff#669, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).diffValue AS diffValue#670, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).deviceName, true, false) AS deviceName#671, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).deviceIp, true, false) AS deviceIp#672, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifName, true, false) AS ifName#673, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifDesc, true, false) AS ifDesc#674, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).indicatorName, true, false) AS indicatorName#675, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).objectName, true, false) AS objectName#676, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).division, true, false) AS division#677, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).sourceType, true, false) AS sourceType#678, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).prevTime AS prevTime#679L, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).vendor, true, false) AS vendor#680, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).model, true, false) AS model#681, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborFqdn, true, false) AS neighborFqdn#682, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborIfName, true, false) AS neighborIfName#683, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborIp, true, false) AS neighborIp#684, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifIndex AS ifIndex#685, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).cpuType, true, false) AS cpuType#686, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).cpuName, true, false) AS cpuName#687, ... 8 more fields]
                        +- MapElements yet.another.company.snmp.dataframe.SnmpDatasetUtils$$$Lambda$1606/815707130@7cc98f8d, class yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, [StructField(uuid,StringType,true), StructField(id,StringType,true), StructField(currValue,DecimalType(38,18),true), StructField(time,LongType,false), StructField(updated,BooleanType,false), StructField(isDiff,BooleanType,false), StructField(diffValue,DoubleType,false), StructField(deviceName,StringType,true), StructField(deviceIp,StringType,true), StructField(ifName,StringType,true), StructField(ifDesc,StringType,true), StructField(indicatorName,StringType,true), StructField(objectName,StringType,true), StructField(division,StringType,true), StructField(sourceType,StringType,true), StructField(prevTime,LongType,false), StructField(vendor,StringType,true), StructField(model,StringType,true), StructField(neighborFqdn,StringType,true), StructField(neighborIfName,StringType,true), StructField(neighborIp,StringType,true), StructField(ifIndex,IntegerType,false), StructField(cpuType,StringType,true), StructField(cpuName,StringType,true), ... 8 more fields], obj#663: yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState
                           +- DeserializeToObject newInstance(class yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState), obj#662: yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState
                              +- SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).uuid, true, false) AS uuid#565, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).id, true, false) AS id#566, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).currValue AS currValue#567, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).time AS time#568L, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).updated AS updated#569, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).isDiff AS isDiff#570, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).diffValue AS diffValue#571, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).deviceName, true, false) AS deviceName#572, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).deviceIp, true, false) AS deviceIp#573, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifName, true, false) AS ifName#574, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifDesc, true, false) AS ifDesc#575, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).indicatorName, true, false) AS indicatorName#576, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).objectName, true, false) AS objectName#577, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).division, true, false) AS division#578, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).sourceType, true, false) AS sourceType#579, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).prevTime AS prevTime#580L, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).vendor, true, false) AS vendor#581, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).model, true, false) AS model#582, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborFqdn, true, false) AS neighborFqdn#583, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborIfName, true, false) AS neighborIfName#584, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborIp, true, false) AS neighborIp#585, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifIndex AS ifIndex#586, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).cpuType, true, false) AS cpuType#587, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).cpuName, true, false) AS cpuName#588, ... 8 more fields]
                                 +- MapElements yet.another.company.core.metrics.LongValStatsAccumulator$$Lambda$1604/1488213611@a4c837a, class yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, [StructField(uuid,StringType,true), StructField(id,StringType,true), StructField(currValue,DecimalType(38,18),true), StructField(time,LongType,false), StructField(updated,BooleanType,false), StructField(isDiff,BooleanType,false), StructField(diffValue,DoubleType,false), StructField(deviceName,StringType,true), StructField(deviceIp,StringType,true), StructField(ifName,StringType,true), StructField(ifDesc,StringType,true), StructField(indicatorName,StringType,true), StructField(objectName,StringType,true), StructField(division,StringType,true), StructField(sourceType,StringType,true), StructField(prevTime,LongType,false), StructField(vendor,StringType,true), StructField(model,StringType,true), StructField(neighborFqdn,StringType,true), StructField(neighborIfName,StringType,true), StructField(neighborIp,StringType,true), StructField(ifIndex,IntegerType,false), StructField(cpuType,StringType,true), StructField(cpuName,StringType,true), ... 8 more fields], obj#564: yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState
                                    +- DeserializeToObject newInstance(class yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState), obj#563: yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState
                                       +- TypedFilter yet.another.company.snmp.dataframe.SnmpDatasetUtils$$$Lambda$1601/1495253571@568e4e8, class yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, [StructField(uuid,StringType,true), StructField(id,StringType,true), StructField(currValue,DecimalType(38,18),true), StructField(time,LongType,false), StructField(updated,BooleanType,false), StructField(isDiff,BooleanType,false), StructField(diffValue,DoubleType,false), StructField(deviceName,StringType,true), StructField(deviceIp,StringType,true), StructField(ifName,StringType,true), StructField(ifDesc,StringType,true), StructField(indicatorName,StringType,true), StructField(objectName,StringType,true), StructField(division,StringType,true), StructField(sourceType,StringType,true), StructField(prevTime,LongType,false), StructField(vendor,StringType,true), StructField(model,StringType,true), StructField(neighborFqdn,StringType,true), StructField(neighborIfName,StringType,true), StructField(neighborIp,StringType,true), StructField(ifIndex,IntegerType,false), StructField(cpuType,StringType,true), StructField(cpuName,StringType,true), ... 8 more fields], newInstance(class yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState)
                                          +- SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).uuid, true, false) AS uuid#433, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).id, true, false) AS id#434, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).currValue AS currValue#435, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).time AS time#436L, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).updated AS updated#437, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).isDiff AS isDiff#438, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).diffValue AS diffValue#439, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).deviceName, true, false) AS deviceName#440, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).deviceIp, true, false) AS deviceIp#441, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifName, true, false) AS ifName#442, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifDesc, true, false) AS ifDesc#443, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).indicatorName, true, false) AS indicatorName#444, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).objectName, true, false) AS objectName#445, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).division, true, false) AS division#446, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).sourceType, true, false) AS sourceType#447, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).prevTime AS prevTime#448L, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).vendor, true, false) AS vendor#449, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).model, true, false) AS model#450, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborFqdn, true, false) AS neighborFqdn#451, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborIfName, true, false) AS neighborIfName#452, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborIp, true, false) AS neighborIp#453, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifIndex AS ifIndex#454, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).cpuType, true, false) AS cpuType#455, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).cpuName, true, false) AS cpuName#456, ... 8 more fields]
                                             +- MapElements yet.another.company.snmp.dataframe.SnmpDatasetUtils$$$Lambda$1600/1656647258@3745c3d6, class yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, [StructField(uuid,StringType,true), StructField(id,StringType,true), StructField(currValue,DecimalType(38,18),true), StructField(time,LongType,false), StructField(updated,BooleanType,false), StructField(isDiff,BooleanType,false), StructField(diffValue,DoubleType,false), StructField(deviceName,StringType,true), StructField(deviceIp,StringType,true), StructField(ifName,StringType,true), StructField(ifDesc,StringType,true), StructField(indicatorName,StringType,true), StructField(objectName,StringType,true), StructField(division,StringType,true), StructField(sourceType,StringType,true), StructField(prevTime,LongType,false), StructField(vendor,StringType,true), StructField(model,StringType,true), StructField(neighborFqdn,StringType,true), StructField(neighborIfName,StringType,true), StructField(neighborIp,StringType,true), StructField(ifIndex,IntegerType,false), StructField(cpuType,StringType,true), StructField(cpuName,StringType,true), ... 8 more fields], obj#432: yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState
                                                +- DeserializeToObject newInstance(class yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState), obj#431: yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState
                                                   +- SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).uuid, true, false) AS uuid#334, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).id, true, false) AS id#335, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).currValue AS currValue#336, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).time AS time#337L, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).updated AS updated#338, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).isDiff AS isDiff#339, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).diffValue AS diffValue#340, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).deviceName, true, false) AS deviceName#341, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).deviceIp, true, false) AS deviceIp#342, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifName, true, false) AS ifName#343, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifDesc, true, false) AS ifDesc#344, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).indicatorName, true, false) AS indicatorName#345, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).objectName, true, false) AS objectName#346, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).division, true, false) AS division#347, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).sourceType, true, false) AS sourceType#348, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).prevTime AS prevTime#349L, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).vendor, true, false) AS vendor#350, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).model, true, false) AS model#351, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborFqdn, true, false) AS neighborFqdn#352, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborIfName, true, false) AS neighborIfName#353, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborIp, true, false) AS neighborIp#354, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifIndex AS ifIndex#355, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).cpuType, true, false) AS cpuType#356, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).cpuName, true, false) AS cpuName#357, ... 8 more fields]
                                                      +- FlatMapGroupsWithState yet.another.company.snmp.dataframe.SnmpDatasetUtils$$$Lambda$1592/535827805@7c09754, cast(value#268 as string).toString, newInstance(class yet.another.company.snmp.dto.RichSnmpRecord), [value#268], [deviceId#242, deviceName#243, deviceIp#244, objectName#245, ifDesc#246, indicatorName#247, value#248, time#249L, pluginName#250, vendor#251, model#252, neighborFqdn#253, neighborIfName#254, neighborIp#255, ifIndex#256, id#257, sourceType#258, cpuType#259, cpuName#260, ifName#261, division#262, metric#263, status#264, tags#265, historyRecords#266], obj#333: yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, class[uuid[0]: string, id[0]: string, currValue[0]: decimal(38,18), time[0]: bigint, updated[0]: boolean, isDiff[0]: boolean, diffValue[0]: double, deviceName[0]: string, deviceIp[0]: string, ifName[0]: string, ifDesc[0]: string, indicatorName[0]: string, objectName[0]: string, division[0]: string, sourceType[0]: string, prevTime[0]: bigint, vendor[0]: string, model[0]: string, neighborFqdn[0]: string, neighborIfName[0]: string, neighborIp[0]: string, ifIndex[0]: int, cpuType[0]: string, cpuName[0]: string, status[0]: struct<underMaintenance:boolean,inInventory:boolean,neighborUnderMaintenance:boolean>, metric[0]: string, tags[0]: map<string,string>, sysId[0]: string, rate[0]: double, history[0]: array<struct<time:bigint,value:decimal(38,18)>>, metaInfo[0]: struct<batchId:bigint>, historyRecords[0]: array<struct<ruleId:string,ruleDescription:string,workflowId:string,className:string,nodeIp:string,eventTime:string>>], Update, false, NoTimeout
                                                         +- AppendColumns yet.another.company.snmp.dataframe.SnmpDatasetUtils$$$Lambda$1590/387698061@69f99385, class yet.another.company.snmp.dto.RichSnmpRecord, [StructField(deviceId,IntegerType,false), StructField(deviceName,StringType,true), StructField(deviceIp,StringType,true), StructField(objectName,StringType,true), StructField(ifDesc,StringType,true), StructField(indicatorName,StringType,true), StructField(value,DecimalType(38,18),true), StructField(time,LongType,false), StructField(pluginName,StringType,true), StructField(vendor,StringType,true), StructField(model,StringType,true), StructField(neighborFqdn,StringType,true), StructField(neighborIfName,StringType,true), StructField(neighborIp,StringType,true), StructField(ifIndex,IntegerType,false), StructField(id,StringType,true), StructField(sourceType,StringType,true), StructField(cpuType,StringType,true), StructField(cpuName,StringType,true), StructField(ifName,StringType,true), StructField(division,StringType,true), StructField(metric,StringType,true), StructField(status,StructType(StructField(underMaintenance,BooleanType,false), StructField(inInventory,BooleanType,false), StructField(neighborUnderMaintenance,BooleanType,true)),true), StructField(tags,MapType(StringType,StringType,true),true), StructField(historyRecords,ArrayType(StructType(StructField(ruleId,StringType,true), StructField(ruleDescription,StringType,true), StructField(workflowId,StringType,true), StructField(className,StringType,true), StructField(nodeIp,StringType,true), StructField(eventTime,StringType,true)),true),true)], newInstance(class yet.another.company.snmp.dto.RichSnmpRecord), [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, java.lang.String, true], true, false) AS value#268]
                                                            +- SerializeFromObject [knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).deviceId AS deviceId#242, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).deviceName, true, false) AS deviceName#243, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).deviceIp, true, false) AS deviceIp#244, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).objectName, true, false) AS objectName#245, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).ifDesc, true, false) AS ifDesc#246, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).indicatorName, true, false) AS indicatorName#247, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).value AS value#248, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).time AS time#249L, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).pluginName, true, false) AS pluginName#250, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).vendor, true, false) AS vendor#251, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).model, true, false) AS model#252, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).neighborFqdn, true, false) AS neighborFqdn#253, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).neighborIfName, true, false) AS neighborIfName#254, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).neighborIp, true, false) AS neighborIp#255, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).ifIndex AS ifIndex#256, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).id, true, false) AS id#257, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).sourceType, true, false) AS sourceType#258, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).cpuType, true, false) AS cpuType#259, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).cpuName, true, false) AS cpuName#260, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).ifName, true, false) AS ifName#261, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).division, true, false) AS division#262, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).metric, true, false) AS metric#263, if (isnull(knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).status)) null else named_struct(underMaintenance, knownnotnull(knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).status).underMaintenance, inInventory, knownnotnull(knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).status).inInventory, neighborUnderMaintenance, unwrapoption(BooleanType, knownnotnull(knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).status).neighborUnderMaintenance)) AS status#264, externalmaptocatalyst(lambdavariable(ExternalMapToCatalyst_key, ObjectType(class java.lang.String), true, 4), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, lambdavariable(ExternalMapToCatalyst_key, ObjectType(class java.lang.String), true, 4), true, false), lambdavariable(ExternalMapToCatalyst_value, ObjectType(class java.lang.String), true, 5), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, lambdavariable(ExternalMapToCatalyst_value, ObjectType(class java.lang.String), true, 5), true, false), knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).tags) AS tags#265, mapobjects(lambdavariable(MapObject, ObjectType(class yet.another.company.core.dto.ProcessingPoint), true, 6), if (isnull(lambdavariable(MapObject, ObjectType(class yet.another.company.core.dto.ProcessingPoint), true, 6))) null else named_struct(ruleId, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(lambdavariable(MapObject, ObjectType(class yet.another.company.core.dto.ProcessingPoint), true, 6)).ruleId, true, false), ruleDescription, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(lambdavariable(MapObject, ObjectType(class yet.another.company.core.dto.ProcessingPoint), true, 6)).ruleDescription, true, false), workflowId, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(lambdavariable(MapObject, ObjectType(class yet.another.company.core.dto.ProcessingPoint), true, 6)).workflowId, true, false), className, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(lambdavariable(MapObject, ObjectType(class yet.another.company.core.dto.ProcessingPoint), true, 6)).className, true, false), nodeIp, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(lambdavariable(MapObject, ObjectType(class yet.another.company.core.dto.ProcessingPoint), true, 6)).nodeIp, true, false), eventTime, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(lambdavariable(MapObject, ObjectType(class yet.another.company.core.dto.ProcessingPoint), true, 6)).eventTime, true, false)), knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).historyRecords, None) AS historyRecords#266]
                                                               +- MapPartitions org.apache.spark.sql.Dataset$$Lambda$1380/595814978@2b7475f6, obj#241: yet.another.company.snmp.dto.RichSnmpRecord
                                                                  +- DeserializeToObject newInstance(class yet.another.company.snmp.dto.RichSnmpRecord), obj#240: yet.another.company.snmp.dto.RichSnmpRecord
                                                                     +- SerializeFromObject [knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).deviceId AS deviceId#190, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).deviceName, true, false) AS deviceName#191, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).deviceIp, true, false) AS deviceIp#192, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).objectName, true, false) AS objectName#193, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).ifDesc, true, false) AS ifDesc#194, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).indicatorName, true, false) AS indicatorName#195, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).value AS value#196, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).time AS time#197L, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).pluginName, true, false) AS pluginName#198, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).vendor, true, false) AS vendor#199, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).model, true, false) AS model#200, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).neighborFqdn, true, false) AS neighborFqdn#201, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).neighborIfName, true, false) AS neighborIfName#202, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).neighborIp, true, false) AS neighborIp#203, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).ifIndex AS ifIndex#204, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).id, true, false) AS id#205, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).sourceType, true, false) AS sourceType#206, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).cpuType, true, false) AS cpuType#207, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).cpuName, true, false) AS cpuName#208, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).ifName, true, false) AS ifName#209, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).division, true, false) AS division#210, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).metric, true, false) AS metric#211, if (isnull(knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).status)) null else named_struct(underMaintenance, knownnotnull(knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).status).underMaintenance, inInventory, knownnotnull(knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).status).inInventory, neighborUnderMaintenance, unwrapoption(BooleanType, knownnotnull(knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).status).neighborUnderMaintenance)) AS status#212, externalmaptocatalyst(lambdavariable(ExternalMapToCatalyst_key, ObjectType(class java.lang.String), true, 1), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, lambdavariable(ExternalMapToCatalyst_key, ObjectType(class java.lang.String), true, 1), true, false), lambdavariable(ExternalMapToCatalyst_value, ObjectType(class java.lang.String), true, 2), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, lambdavariable(ExternalMapToCatalyst_value, ObjectType(class java.lang.String), true, 2), true, false), knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).tags) AS tags#213, mapobjects(lambdavariable(MapObject, ObjectType(class yet.another.company.core.dto.ProcessingPoint), true, 3), if (isnull(lambdavariable(MapObject, ObjectType(class yet.another.company.core.dto.ProcessingPoint), true, 3))) null else named_struct(ruleId, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(lambdavariable(MapObject, ObjectType(class yet.another.company.core.dto.ProcessingPoint), true, 3)).ruleId, true, false), ruleDescription, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(lambdavariable(MapObject, ObjectType(class yet.another.company.core.dto.ProcessingPoint), true, 3)).ruleDescription, true, false), workflowId, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(lambdavariable(MapObject, ObjectType(class yet.another.company.core.dto.ProcessingPoint), true, 3)).workflowId, true, false), className, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(lambdavariable(MapObject, ObjectType(class yet.another.company.core.dto.ProcessingPoint), true, 3)).className, true, false), nodeIp, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(lambdavariable(MapObject, ObjectType(class yet.another.company.core.dto.ProcessingPoint), true, 3)).nodeIp, true, false), eventTime, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(lambdavariable(MapObject, ObjectType(class yet.another.company.core.dto.ProcessingPoint), true, 3)).eventTime, true, false)), knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).historyRecords, None) AS historyRecords#214]
                                                                        +- MapPartitions org.apache.spark.sql.Dataset$$Lambda$1380/595814978@3236483d, obj#189: yet.another.company.snmp.dto.RichSnmpRecord
                                                                           +- DeserializeToObject newInstance(class yet.another.company.snmp.streaming.InputSevOneRecord), obj#188: yet.another.company.snmp.streaming.InputSevOneRecord
                                                                              +- Union
                                                                                 :- TypedFilter yet.another.company.snmp.streaming.SnmpStreaming$$$Lambda$1397/1187498311@314f1318, class yet.another.company.snmp.streaming.InputSevOneRecord, [StructField(deviceId,IntegerType,false), StructField(deviceName,StringType,true), StructField(deviceIp,StringType,true), StructField(peerId,IntegerType,false), StructField(objectId,IntegerType,false), StructField(objectName,StringType,true), StructField(objectDesc,StringType,true), StructField(pluginId,IntegerType,false), StructField(pluginName,StringType,true), StructField(indicatorId,IntegerType,false), StructField(indicatorName,StringType,true), StructField(format,IntegerType,false), StructField(value,StringType,true), StructField(time,DoubleType,false), StructField(clusterName,StringType,true), StructField(peerIp,StringType,true)], newInstance(class yet.another.company.snmp.streaming.InputSevOneRecord)
                                                                                 :  +- SerializeFromObject [knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).deviceId AS deviceId#40, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).deviceName, true, false) AS deviceName#41, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).deviceIp, true, false) AS deviceIp#42, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).peerId AS peerId#43, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).objectId AS objectId#44, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).objectName, true, false) AS objectName#45, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).objectDesc, true, false) AS objectDesc#46, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).pluginId AS pluginId#47, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).pluginName, true, false) AS pluginName#48, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).indicatorId AS indicatorId#49, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).indicatorName, true, false) AS indicatorName#50, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).format AS format#51, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).value, true, false) AS value#52, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).time AS time#53, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).clusterName, true, false) AS clusterName#54, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).peerIp, true, false) AS peerIp#55]
                                                                                 :     +- MapPartitions org.apache.spark.sql.Dataset$$Lambda$1380/595814978@148fdb0f, obj#39: yet.another.company.snmp.streaming.InputSevOneRecord
                                                                                 :        +- DeserializeToObject cast(value#8 as binary), obj#38: binary
                                                                                 :           +- Project [value#8]
                                                                                 :              +- StreamingDataSourceV2Relation [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan@1c5cc7ad, KafkaV2[Subscribe[sevone-spdb]]
                                                                                 +- TypedFilter yet.another.company.snmp.streaming.SnmpStreaming$$$Lambda$1397/1187498311@314f1318, class yet.another.company.snmp.streaming.InputSevOneRecord, [StructField(deviceId,IntegerType,false), StructField(deviceName,StringType,true), StructField(deviceIp,StringType,true), StructField(peerId,IntegerType,false), StructField(objectId,IntegerType,false), StructField(objectName,StringType,true), StructField(objectDesc,StringType,true), StructField(pluginId,IntegerType,false), StructField(pluginName,StringType,true), StructField(indicatorId,IntegerType,false), StructField(indicatorName,StringType,true), StructField(format,IntegerType,false), StructField(value,StringType,true), StructField(time,DoubleType,false), StructField(clusterName,StringType,true), StructField(peerIp,StringType,true)], newInstance(class yet.another.company.snmp.streaming.InputSevOneRecord)
                                                                                    +- SerializeFromObject [knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).deviceId AS deviceId#113, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).deviceName, true, false) AS deviceName#114, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).deviceIp, true, false) AS deviceIp#115, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).peerId AS peerId#116, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).objectId AS objectId#117, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).objectName, true, false) AS objectName#118, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).objectDesc, true, false) AS objectDesc#119, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).pluginId AS pluginId#120, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).pluginName, true, false) AS pluginName#121, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).indicatorId AS indicatorId#122, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).indicatorName, true, false) AS indicatorName#123, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).format AS format#124, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).value, true, false) AS value#125, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).time AS time#126, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).clusterName, true, false) AS clusterName#127, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).peerIp, true, false) AS peerIp#128]
                                                                                       +- MapPartitions org.apache.spark.sql.Dataset$$Lambda$1380/595814978@1c2b13bb, obj#112: yet.another.company.snmp.streaming.InputSevOneRecord
                                                                                          +- DeserializeToObject cast(value#81 as binary), obj#111: binary
                                                                                             +- Project [value#81]
                                                                                                +- StreamingDataSourceV2Relation [key#80, value#81, topic#82, partition#83, offset#84L, timestamp#85, timestampType#86], org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan@553514f5, KafkaV2[Subscribe[sevone-spdb]]

	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:355)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:245)
Caused by: org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-497959199-96.113.30.135-1607028926403:blk_1073795726_55426 file=/snmp-checkpoints/offsets/128
	at org.apache.hadoop.hdfs.DFSInputStream.refetchLocations(DFSInputStream.java:879)
	at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:862)
	at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:841)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:567)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:757)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:829)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	at java.io.InputStreamReader.read(InputStreamReader.java:184)
	at java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.io.BufferedReader.readLine(BufferedReader.java:324)
	at java.io.BufferedReader.readLine(BufferedReader.java:389)
	at scala.io.BufferedSource$BufferedLineIterator.hasNext(BufferedSource.scala:74)
	at org.apache.spark.sql.execution.streaming.OffsetSeqLog.deserialize(OffsetSeqLog.scala:56)
	at org.apache.spark.sql.execution.streaming.OffsetSeqLog.deserialize(OffsetSeqLog.scala:46)
	at org.apache.spark.sql.execution.streaming.HDFSMetadataLog.get(HDFSMetadataLog.scala:153)
	at org.apache.spark.sql.execution.streaming.HDFSMetadataLog.$anonfun$getLatest$2(HDFSMetadataLog.scala:190)
	at scala.runtime.java8.JFunction1$mcVJ$sp.apply(JFunction1$mcVJ$sp.java:23)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofLong.foreach(ArrayOps.scala:258)
	at org.apache.spark.sql.execution.streaming.HDFSMetadataLog.getLatest(HDFSMetadataLog.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.populateStartOffsets(MicroBatchExecution.scala:272)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:194)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:352)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:350)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:191)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:185)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:334)
	... 1 more

	 ApplicationMaster host: ip-96-113-30-118.nest.r53.xcal.tv
	 ApplicationMaster RPC port: 33667
	 queue: default
	 start time: 1607340808025
	 final status: FAILED
	 tracking URL: http://ip-96-113-30-135.nest.r53.xcal.tv:20888/proxy/application_1607028964151_1511/
	 user: hadoop
20/12/07 11:33:56 ERROR Client: Application diagnostics message: Diagnostic messages truncated, showing last 65536 chars out of 75821:
...e])).indicatorName, true, false) AS indicatorName#774, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).objectName, true, false) AS objectName#775, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).division, true, false) AS division#776, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).sourceType, true, false) AS sourceType#777, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).prevTime AS prevTime#778L, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).vendor, true, false) AS vendor#779, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).model, true, false) AS model#780, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborFqdn, true, false) AS neighborFqdn#781, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborIfName, true, false) AS neighborIfName#782, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborIp, true, false) AS neighborIp#783, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifIndex AS ifIndex#784, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).cpuType, true, false) AS cpuType#785, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).cpuName, true, false) AS cpuName#786, ... 8 more fields]
               +- MapElements yet.another.company.snmp.dataframe.SnmpDatasetUtils$$$Lambda$1609/1385079241@411e703b, class yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, [StructField(uuid,StringType,true), StructField(id,StringType,true), StructField(currValue,DecimalType(38,18),true), StructField(time,LongType,false), StructField(updated,BooleanType,false), StructField(isDiff,BooleanType,false), StructField(diffValue,DoubleType,false), StructField(deviceName,StringType,true), StructField(deviceIp,StringType,true), StructField(ifName,StringType,true), StructField(ifDesc,StringType,true), StructField(indicatorName,StringType,true), StructField(objectName,StringType,true), StructField(division,StringType,true), StructField(sourceType,StringType,true), StructField(prevTime,LongType,false), StructField(vendor,StringType,true), StructField(model,StringType,true), StructField(neighborFqdn,StringType,true), StructField(neighborIfName,StringType,true), StructField(neighborIp,StringType,true), StructField(ifIndex,IntegerType,false), StructField(cpuType,StringType,true), StructField(cpuName,StringType,true), ... 8 more fields], obj#762: yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState
                  +- DeserializeToObject newInstance(class yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState), obj#761: yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState
                     +- SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).uuid, true, false) AS uuid#664, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).id, true, false) AS id#665, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).currValue AS currValue#666, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).time AS time#667L, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).updated AS updated#668, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).isDiff AS isDiff#669, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).diffValue AS diffValue#670, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).deviceName, true, false) AS deviceName#671, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).deviceIp, true, false) AS deviceIp#672, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifName, true, false) AS ifName#673, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifDesc, true, false) AS ifDesc#674, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).indicatorName, true, false) AS indicatorName#675, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).objectName, true, false) AS objectName#676, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).division, true, false) AS division#677, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).sourceType, true, false) AS sourceType#678, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).prevTime AS prevTime#679L, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).vendor, true, false) AS vendor#680, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).model, true, false) AS model#681, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborFqdn, true, false) AS neighborFqdn#682, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborIfName, true, false) AS neighborIfName#683, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborIp, true, false) AS neighborIp#684, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifIndex AS ifIndex#685, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).cpuType, true, false) AS cpuType#686, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).cpuName, true, false) AS cpuName#687, ... 8 more fields]
                        +- MapElements yet.another.company.snmp.dataframe.SnmpDatasetUtils$$$Lambda$1606/815707130@7cc98f8d, class yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, [StructField(uuid,StringType,true), StructField(id,StringType,true), StructField(currValue,DecimalType(38,18),true), StructField(time,LongType,false), StructField(updated,BooleanType,false), StructField(isDiff,BooleanType,false), StructField(diffValue,DoubleType,false), StructField(deviceName,StringType,true), StructField(deviceIp,StringType,true), StructField(ifName,StringType,true), StructField(ifDesc,StringType,true), StructField(indicatorName,StringType,true), StructField(objectName,StringType,true), StructField(division,StringType,true), StructField(sourceType,StringType,true), StructField(prevTime,LongType,false), StructField(vendor,StringType,true), StructField(model,StringType,true), StructField(neighborFqdn,StringType,true), StructField(neighborIfName,StringType,true), StructField(neighborIp,StringType,true), StructField(ifIndex,IntegerType,false), StructField(cpuType,StringType,true), StructField(cpuName,StringType,true), ... 8 more fields], obj#663: yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState
                           +- DeserializeToObject newInstance(class yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState), obj#662: yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState
                              +- SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).uuid, true, false) AS uuid#565, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).id, true, false) AS id#566, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).currValue AS currValue#567, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).time AS time#568L, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).updated AS updated#569, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).isDiff AS isDiff#570, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).diffValue AS diffValue#571, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).deviceName, true, false) AS deviceName#572, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).deviceIp, true, false) AS deviceIp#573, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifName, true, false) AS ifName#574, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifDesc, true, false) AS ifDesc#575, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).indicatorName, true, false) AS indicatorName#576, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).objectName, true, false) AS objectName#577, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).division, true, false) AS division#578, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).sourceType, true, false) AS sourceType#579, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).prevTime AS prevTime#580L, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).vendor, true, false) AS vendor#581, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).model, true, false) AS model#582, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborFqdn, true, false) AS neighborFqdn#583, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborIfName, true, false) AS neighborIfName#584, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborIp, true, false) AS neighborIp#585, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifIndex AS ifIndex#586, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).cpuType, true, false) AS cpuType#587, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).cpuName, true, false) AS cpuName#588, ... 8 more fields]
                                 +- MapElements yet.another.company.core.metrics.LongValStatsAccumulator$$Lambda$1604/1488213611@a4c837a, class yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, [StructField(uuid,StringType,true), StructField(id,StringType,true), StructField(currValue,DecimalType(38,18),true), StructField(time,LongType,false), StructField(updated,BooleanType,false), StructField(isDiff,BooleanType,false), StructField(diffValue,DoubleType,false), StructField(deviceName,StringType,true), StructField(deviceIp,StringType,true), StructField(ifName,StringType,true), StructField(ifDesc,StringType,true), StructField(indicatorName,StringType,true), StructField(objectName,StringType,true), StructField(division,StringType,true), StructField(sourceType,StringType,true), StructField(prevTime,LongType,false), StructField(vendor,StringType,true), StructField(model,StringType,true), StructField(neighborFqdn,StringType,true), StructField(neighborIfName,StringType,true), StructField(neighborIp,StringType,true), StructField(ifIndex,IntegerType,false), StructField(cpuType,StringType,true), StructField(cpuName,StringType,true), ... 8 more fields], obj#564: yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState
                                    +- DeserializeToObject newInstance(class yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState), obj#563: yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState
                                       +- TypedFilter yet.another.company.snmp.dataframe.SnmpDatasetUtils$$$Lambda$1601/1495253571@568e4e8, class yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, [StructField(uuid,StringType,true), StructField(id,StringType,true), StructField(currValue,DecimalType(38,18),true), StructField(time,LongType,false), StructField(updated,BooleanType,false), StructField(isDiff,BooleanType,false), StructField(diffValue,DoubleType,false), StructField(deviceName,StringType,true), StructField(deviceIp,StringType,true), StructField(ifName,StringType,true), StructField(ifDesc,StringType,true), StructField(indicatorName,StringType,true), StructField(objectName,StringType,true), StructField(division,StringType,true), StructField(sourceType,StringType,true), StructField(prevTime,LongType,false), StructField(vendor,StringType,true), StructField(model,StringType,true), StructField(neighborFqdn,StringType,true), StructField(neighborIfName,StringType,true), StructField(neighborIp,StringType,true), StructField(ifIndex,IntegerType,false), StructField(cpuType,StringType,true), StructField(cpuName,StringType,true), ... 8 more fields], newInstance(class yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState)
                                          +- SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).uuid, true, false) AS uuid#433, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).id, true, false) AS id#434, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).currValue AS currValue#435, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).time AS time#436L, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).updated AS updated#437, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).isDiff AS isDiff#438, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).diffValue AS diffValue#439, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).deviceName, true, false) AS deviceName#440, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).deviceIp, true, false) AS deviceIp#441, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifName, true, false) AS ifName#442, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifDesc, true, false) AS ifDesc#443, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).indicatorName, true, false) AS indicatorName#444, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).objectName, true, false) AS objectName#445, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).division, true, false) AS division#446, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).sourceType, true, false) AS sourceType#447, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).prevTime AS prevTime#448L, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).vendor, true, false) AS vendor#449, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).model, true, false) AS model#450, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborFqdn, true, false) AS neighborFqdn#451, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborIfName, true, false) AS neighborIfName#452, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborIp, true, false) AS neighborIp#453, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifIndex AS ifIndex#454, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).cpuType, true, false) AS cpuType#455, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).cpuName, true, false) AS cpuName#456, ... 8 more fields]
                                             +- MapElements yet.another.company.snmp.dataframe.SnmpDatasetUtils$$$Lambda$1600/1656647258@3745c3d6, class yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, [StructField(uuid,StringType,true), StructField(id,StringType,true), StructField(currValue,DecimalType(38,18),true), StructField(time,LongType,false), StructField(updated,BooleanType,false), StructField(isDiff,BooleanType,false), StructField(diffValue,DoubleType,false), StructField(deviceName,StringType,true), StructField(deviceIp,StringType,true), StructField(ifName,StringType,true), StructField(ifDesc,StringType,true), StructField(indicatorName,StringType,true), StructField(objectName,StringType,true), StructField(division,StringType,true), StructField(sourceType,StringType,true), StructField(prevTime,LongType,false), StructField(vendor,StringType,true), StructField(model,StringType,true), StructField(neighborFqdn,StringType,true), StructField(neighborIfName,StringType,true), StructField(neighborIp,StringType,true), StructField(ifIndex,IntegerType,false), StructField(cpuType,StringType,true), StructField(cpuName,StringType,true), ... 8 more fields], obj#432: yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState
                                                +- DeserializeToObject newInstance(class yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState), obj#431: yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState
                                                   +- SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).uuid, true, false) AS uuid#334, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).id, true, false) AS id#335, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).currValue AS currValue#336, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).time AS time#337L, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).updated AS updated#338, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).isDiff AS isDiff#339, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).diffValue AS diffValue#340, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).deviceName, true, false) AS deviceName#341, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).deviceIp, true, false) AS deviceIp#342, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifName, true, false) AS ifName#343, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifDesc, true, false) AS ifDesc#344, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).indicatorName, true, false) AS indicatorName#345, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).objectName, true, false) AS objectName#346, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).division, true, false) AS division#347, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assCommand exiting with ret '1'

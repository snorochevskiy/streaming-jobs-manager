Ivy Default Cache set to: /home/hadoop/.ivy2/cache
The jars for the packages stored in: /home/hadoop/.ivy2/jars
:: loading settings :: url = jar:file:/usr/lib/spark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
com.qubole.spark#spark-sql-kinesis_2.12 added as a dependency
mysql#mysql-connector-java added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-ddd0331e-a8b1-4d71-a0b6-89c8346d07da;1.0
	confs: [default]
	found org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 in central
	found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 in central
	found org.apache.kafka#kafka-clients;2.4.1 in central
	found com.github.luben#zstd-jni;1.4.4-3 in central
	found org.lz4#lz4-java;1.7.1 in central
	found org.xerial.snappy#snappy-java;1.1.7.5 in central
	found org.slf4j#slf4j-api;1.7.30 in central
	found org.spark-project.spark#unused;1.0.0 in central
	found org.apache.commons#commons-pool2;2.6.2 in central
	found com.qubole.spark#spark-sql-kinesis_2.12;1.2.0_spark-3.0 in central
	found mysql#mysql-connector-java;8.0.20 in central
	found com.google.protobuf#protobuf-java;3.6.1 in central
:: resolution report :: resolve 283ms :: artifacts dl 6ms
	:: modules in use:
	com.github.luben#zstd-jni;1.4.4-3 from central in [default]
	com.google.protobuf#protobuf-java;3.6.1 from central in [default]
	com.qubole.spark#spark-sql-kinesis_2.12;1.2.0_spark-3.0 from central in [default]
	mysql#mysql-connector-java;8.0.20 from central in [default]
	org.apache.commons#commons-pool2;2.6.2 from central in [default]
	org.apache.kafka#kafka-clients;2.4.1 from central in [default]
	org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 from central in [default]
	org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 from central in [default]
	org.lz4#lz4-java;1.7.1 from central in [default]
	org.slf4j#slf4j-api;1.7.30 from central in [default]
	org.spark-project.spark#unused;1.0.0 from central in [default]
	org.xerial.snappy#snappy-java;1.1.7.5 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-ddd0331e-a8b1-4d71-a0b6-89c8346d07da
	confs: [default]
	0 artifacts copied, 12 already retrieved (0kB/8ms)
20/11/19 13:52:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/11/19 13:52:34 INFO RMProxy: Connecting to ResourceManager at ip-96-113-30-153.nest.r53.xcal.tv/96.113.30.153:8032
20/11/19 13:52:34 INFO Client: Requesting a new application from cluster with 18 NodeManagers
20/11/19 13:52:34 INFO Configuration: resource-types.xml not found
20/11/19 13:52:34 INFO ResourceUtils: Unable to find 'resource-types.xml'.
20/11/19 13:52:34 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (57344 MB per container)
20/11/19 13:52:34 INFO Client: Will allocate AM container, with 11264 MB memory including 1024 MB overhead
20/11/19 13:52:34 INFO Client: Setting up container launch context for our AM
20/11/19 13:52:34 INFO Client: Setting up the launch environment for our AM container
20/11/19 13:52:34 INFO Client: Preparing resources for our AM container
20/11/19 13:52:34 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/11/19 13:52:36 INFO Client: Uploading resource file:/mnt/tmp/spark-892118ec-feda-4eed-9510-d8264f12b84a/__spark_libs__5147900574634650887.zip -> hdfs://ip-96-113-30-153.nest.r53.xcal.tv:8020/user/hadoop/.sparkStaging/application_1605540135355_0076/__spark_libs__5147900574634650887.zip
20/11/19 13:52:37 INFO Client: Uploading resource file:/home/hadoop/data-valve.snmp-assembly-1.3.1-spark3.jar -> hdfs://ip-96-113-30-153.nest.r53.xcal.tv:8020/user/hadoop/.sparkStaging/application_1605540135355_0076/data-valve.snmp-assembly-1.3.1-spark3.jar
20/11/19 13:52:37 INFO Client: Uploading resource file:/home/hadoop/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.0.0.jar -> hdfs://ip-96-113-30-153.nest.r53.xcal.tv:8020/user/hadoop/.sparkStaging/application_1605540135355_0076/org.apache.spark_spark-sql-kafka-0-10_2.12-3.0.0.jar
20/11/19 13:52:37 INFO Client: Uploading resource file:/home/hadoop/.ivy2/jars/com.qubole.spark_spark-sql-kinesis_2.12-1.2.0_spark-3.0.jar -> hdfs://ip-96-113-30-153.nest.r53.xcal.tv:8020/user/hadoop/.sparkStaging/application_1605540135355_0076/com.qubole.spark_spark-sql-kinesis_2.12-1.2.0_spark-3.0.jar
20/11/19 13:52:37 INFO Client: Uploading resource file:/home/hadoop/.ivy2/jars/mysql_mysql-connector-java-8.0.20.jar -> hdfs://ip-96-113-30-153.nest.r53.xcal.tv:8020/user/hadoop/.sparkStaging/application_1605540135355_0076/mysql_mysql-connector-java-8.0.20.jar
20/11/19 13:52:37 INFO Client: Uploading resource file:/home/hadoop/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.0.0.jar -> hdfs://ip-96-113-30-153.nest.r53.xcal.tv:8020/user/hadoop/.sparkStaging/application_1605540135355_0076/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.0.0.jar
20/11/19 13:52:37 INFO Client: Uploading resource file:/home/hadoop/.ivy2/jars/org.apache.kafka_kafka-clients-2.4.1.jar -> hdfs://ip-96-113-30-153.nest.r53.xcal.tv:8020/user/hadoop/.sparkStaging/application_1605540135355_0076/org.apache.kafka_kafka-clients-2.4.1.jar
20/11/19 13:52:37 INFO Client: Uploading resource file:/home/hadoop/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar -> hdfs://ip-96-113-30-153.nest.r53.xcal.tv:8020/user/hadoop/.sparkStaging/application_1605540135355_0076/org.apache.commons_commons-pool2-2.6.2.jar
20/11/19 13:52:37 INFO Client: Uploading resource file:/home/hadoop/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar -> hdfs://ip-96-113-30-153.nest.r53.xcal.tv:8020/user/hadoop/.sparkStaging/application_1605540135355_0076/org.spark-project.spark_unused-1.0.0.jar
20/11/19 13:52:37 INFO Client: Uploading resource file:/home/hadoop/.ivy2/jars/com.github.luben_zstd-jni-1.4.4-3.jar -> hdfs://ip-96-113-30-153.nest.r53.xcal.tv:8020/user/hadoop/.sparkStaging/application_1605540135355_0076/com.github.luben_zstd-jni-1.4.4-3.jar
20/11/19 13:52:37 INFO Client: Uploading resource file:/home/hadoop/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar -> hdfs://ip-96-113-30-153.nest.r53.xcal.tv:8020/user/hadoop/.sparkStaging/application_1605540135355_0076/org.lz4_lz4-java-1.7.1.jar
20/11/19 13:52:37 INFO Client: Uploading resource file:/home/hadoop/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar -> hdfs://ip-96-113-30-153.nest.r53.xcal.tv:8020/user/hadoop/.sparkStaging/application_1605540135355_0076/org.xerial.snappy_snappy-java-1.1.7.5.jar
20/11/19 13:52:37 INFO Client: Uploading resource file:/home/hadoop/.ivy2/jars/org.slf4j_slf4j-api-1.7.30.jar -> hdfs://ip-96-113-30-153.nest.r53.xcal.tv:8020/user/hadoop/.sparkStaging/application_1605540135355_0076/org.slf4j_slf4j-api-1.7.30.jar
20/11/19 13:52:37 INFO Client: Uploading resource file:/home/hadoop/.ivy2/jars/com.google.protobuf_protobuf-java-3.6.1.jar -> hdfs://ip-96-113-30-153.nest.r53.xcal.tv:8020/user/hadoop/.sparkStaging/application_1605540135355_0076/com.google.protobuf_protobuf-java-3.6.1.jar
20/11/19 13:52:37 INFO Client: Uploading resource file:/mnt/tmp/spark-892118ec-feda-4eed-9510-d8264f12b84a/__spark_conf__5427267458016040066.zip -> hdfs://ip-96-113-30-153.nest.r53.xcal.tv:8020/user/hadoop/.sparkStaging/application_1605540135355_0076/__spark_conf__.zip
20/11/19 13:52:37 INFO SecurityManager: Changing view acls to: hadoop
20/11/19 13:52:37 INFO SecurityManager: Changing modify acls to: hadoop
20/11/19 13:52:37 INFO SecurityManager: Changing view acls groups to: 
20/11/19 13:52:37 INFO SecurityManager: Changing modify acls groups to: 
20/11/19 13:52:37 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hadoop); groups with view permissions: Set(); users  with modify permissions: Set(hadoop); groups with modify permissions: Set()
20/11/19 13:52:37 INFO Client: Submitting application application_1605540135355_0076 to ResourceManager
20/11/19 13:52:37 INFO YarnClientImpl: Submitted application application_1605540135355_0076
20/11/19 13:52:38 INFO Client: Application report for application_1605540135355_0076 (state: ACCEPTED)
20/11/19 13:52:38 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1605793957502
	 final status: UNDEFINED
	 tracking URL: http://ip-96-113-30-153.nest.r53.xcal.tv:20888/proxy/application_1605540135355_0076/
	 user: hadoop
20/11/19 13:52:39 INFO Client: Application report for application_1605540135355_0076 (state: ACCEPTED)
20/11/19 13:52:40 INFO Client: Application report for application_1605540135355_0076 (state: ACCEPTED)
20/11/19 13:52:41 INFO Client: Application report for application_1605540135355_0076 (state: ACCEPTED)
20/11/19 13:52:42 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:52:42 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: ip-96-113-30-250.nest.r53.xcal.tv
	 ApplicationMaster RPC port: 35267
	 queue: default
	 start time: 1605793957502
	 final status: UNDEFINED
	 tracking URL: http://ip-96-113-30-153.nest.r53.xcal.tv:20888/proxy/application_1605540135355_0076/
	 user: hadoop
20/11/19 13:52:43 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:52:44 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:52:45 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:52:46 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:52:47 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:52:48 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:52:49 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:52:50 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:52:51 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:52:52 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:52:53 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:52:54 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:52:55 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:52:56 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:52:57 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:52:58 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:52:59 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:53:00 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:53:01 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:53:02 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:53:03 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:53:04 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:53:05 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:53:06 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:53:07 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:53:08 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:53:09 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:53:10 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:53:11 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:53:12 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:53:13 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:53:14 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:53:15 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:53:16 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:53:17 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:53:18 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:53:19 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:53:20 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:53:21 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:53:22 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:53:23 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:53:24 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:53:25 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:53:26 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:53:27 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:53:28 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:53:29 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:53:30 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:53:31 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:53:32 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:53:33 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:53:34 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:53:35 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:53:36 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:53:37 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:53:38 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:53:39 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:53:40 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:53:41 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:53:42 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:53:43 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:53:44 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:53:45 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:53:46 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:53:47 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:53:48 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:53:49 INFO Client: Application report for application_1605540135355_0076 (state: RUNNING)
20/11/19 13:53:50 INFO Client: Application report for application_1605540135355_0076 (state: FINISHED)
20/11/19 13:53:50 INFO Client: 
	 client token: N/A
	 diagnostics: Diagnostic messages truncated, showing last 65536 chars out of 91225:
...org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborIfName, true, false) AS neighborIfName#683, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborIp, true, false) AS neighborIp#684, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifIndex AS ifIndex#685, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).cpuType, true, false) AS cpuType#686, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).cpuName, true, false) AS cpuName#687, ... 8 more fields]
                        +- MapElements yet.another.company.snmp.dataframe.SnmpDatasetUtils$$$Lambda$1622/839345003@411b1647, class yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, [StructField(uuid,StringType,true), StructField(id,StringType,true), StructField(currValue,DecimalType(38,18),true), StructField(time,LongType,false), StructField(updated,BooleanType,false), StructField(isDiff,BooleanType,false), StructField(diffValue,DoubleType,false), StructField(deviceName,StringType,true), StructField(deviceIp,StringType,true), StructField(ifName,StringType,true), StructField(ifDesc,StringType,true), StructField(indicatorName,StringType,true), StructField(objectName,StringType,true), StructField(division,StringType,true), StructField(sourceType,StringType,true), StructField(prevTime,LongType,false), StructField(vendor,StringType,true), StructField(model,StringType,true), StructField(neighborFqdn,StringType,true), StructField(neighborIfName,StringType,true), StructField(neighborIp,StringType,true), StructField(ifIndex,IntegerType,false), StructField(cpuType,StringType,true), StructField(cpuName,StringType,true), ... 8 more fields], obj#663: yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState
                           +- DeserializeToObject newInstance(class yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState), obj#662: yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState
                              +- SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).uuid, true, false) AS uuid#565, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).id, true, false) AS id#566, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).currValue AS currValue#567, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).time AS time#568L, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).updated AS updated#569, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).isDiff AS isDiff#570, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).diffValue AS diffValue#571, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).deviceName, true, false) AS deviceName#572, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).deviceIp, true, false) AS deviceIp#573, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifName, true, false) AS ifName#574, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifDesc, true, false) AS ifDesc#575, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).indicatorName, true, false) AS indicatorName#576, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).objectName, true, false) AS objectName#577, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).division, true, false) AS division#578, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).sourceType, true, false) AS sourceType#579, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).prevTime AS prevTime#580L, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).vendor, true, false) AS vendor#581, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).model, true, false) AS model#582, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborFqdn, true, false) AS neighborFqdn#583, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborIfName, true, false) AS neighborIfName#584, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborIp, true, false) AS neighborIp#585, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifIndex AS ifIndex#586, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).cpuType, true, false) AS cpuType#587, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).cpuName, true, false) AS cpuName#588, ... 8 more fields]
                                 +- MapElements yet.another.company.core.metrics.LongValStatsAccumulator$$Lambda$1620/469523646@3fafe025, class yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, [StructField(uuid,StringType,true), StructField(id,StringType,true), StructField(currValue,DecimalType(38,18),true), StructField(time,LongType,false), StructField(updated,BooleanType,false), StructField(isDiff,BooleanType,false), StructField(diffValue,DoubleType,false), StructField(deviceName,StringType,true), StructField(deviceIp,StringType,true), StructField(ifName,StringType,true), StructField(ifDesc,StringType,true), StructField(indicatorName,StringType,true), StructField(objectName,StringType,true), StructField(division,StringType,true), StructField(sourceType,StringType,true), StructField(prevTime,LongType,false), StructField(vendor,StringType,true), StructField(model,StringType,true), StructField(neighborFqdn,StringType,true), StructField(neighborIfName,StringType,true), StructField(neighborIp,StringType,true), StructField(ifIndex,IntegerType,false), StructField(cpuType,StringType,true), StructField(cpuName,StringType,true), ... 8 more fields], obj#564: yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState
                                    +- DeserializeToObject newInstance(class yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState), obj#563: yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState
                                       +- TypedFilter yet.another.company.snmp.dataframe.SnmpDatasetUtils$$$Lambda$1617/154986516@6487ffa7, class yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, [StructField(uuid,StringType,true), StructField(id,StringType,true), StructField(currValue,DecimalType(38,18),true), StructField(time,LongType,false), StructField(updated,BooleanType,false), StructField(isDiff,BooleanType,false), StructField(diffValue,DoubleType,false), StructField(deviceName,StringType,true), StructField(deviceIp,StringType,true), StructField(ifName,StringType,true), StructField(ifDesc,StringType,true), StructField(indicatorName,StringType,true), StructField(objectName,StringType,true), StructField(division,StringType,true), StructField(sourceType,StringType,true), StructField(prevTime,LongType,false), StructField(vendor,StringType,true), StructField(model,StringType,true), StructField(neighborFqdn,StringType,true), StructField(neighborIfName,StringType,true), StructField(neighborIp,StringType,true), StructField(ifIndex,IntegerType,false), StructField(cpuType,StringType,true), StructField(cpuName,StringType,true), ... 8 more fields], newInstance(class yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState)
                                          +- SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).uuid, true, false) AS uuid#433, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).id, true, false) AS id#434, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).currValue AS currValue#435, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).time AS time#436L, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).updated AS updated#437, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).isDiff AS isDiff#438, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).diffValue AS diffValue#439, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).deviceName, true, false) AS deviceName#440, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).deviceIp, true, false) AS deviceIp#441, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifName, true, false) AS ifName#442, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifDesc, true, false) AS ifDesc#443, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).indicatorName, true, false) AS indicatorName#444, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).objectName, true, false) AS objectName#445, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).division, true, false) AS division#446, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).sourceType, true, false) AS sourceType#447, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).prevTime AS prevTime#448L, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).vendor, true, false) AS vendor#449, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).model, true, false) AS model#450, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborFqdn, true, false) AS neighborFqdn#451, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborIfName, true, false) AS neighborIfName#452, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborIp, true, false) AS neighborIp#453, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifIndex AS ifIndex#454, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).cpuType, true, false) AS cpuType#455, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).cpuName, true, false) AS cpuName#456, ... 8 more fields]
                                             +- MapElements yet.another.company.snmp.dataframe.SnmpDatasetUtils$$$Lambda$1616/1774441263@7f928ab8, class yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, [StructField(uuid,StringType,true), StructField(id,StringType,true), StructField(currValue,DecimalType(38,18),true), StructField(time,LongType,false), StructField(updated,BooleanType,false), StructField(isDiff,BooleanType,false), StructField(diffValue,DoubleType,false), StructField(deviceName,StringType,true), StructField(deviceIp,StringType,true), StructField(ifName,StringType,true), StructField(ifDesc,StringType,true), StructField(indicatorName,StringType,true), StructField(objectName,StringType,true), StructField(division,StringType,true), StructField(sourceType,StringType,true), StructField(prevTime,LongType,false), StructField(vendor,StringType,true), StructField(model,StringType,true), StructField(neighborFqdn,StringType,true), StructField(neighborIfName,StringType,true), StructField(neighborIp,StringType,true), StructField(ifIndex,IntegerType,false), StructField(cpuType,StringType,true), StructField(cpuName,StringType,true), ... 8 more fields], obj#432: yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState
                                                +- DeserializeToObject newInstance(class yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState), obj#431: yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState
                                                   +- SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).uuid, true, false) AS uuid#334, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).id, true, false) AS id#335, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).currValue AS currValue#336, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).time AS time#337L, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).updated AS updated#338, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).isDiff AS isDiff#339, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).diffValue AS diffValue#340, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).deviceName, true, false) AS deviceName#341, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).deviceIp, true, false) AS deviceIp#342, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifName, true, false) AS ifName#343, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifDesc, true, false) AS ifDesc#344, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).indicatorName, true, false) AS indicatorName#345, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).objectName, true, false) AS objectName#346, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).division, true, false) AS division#347, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).sourceType, true, false) AS sourceType#348, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).prevTime AS prevTime#349L, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).vendor, true, false) AS vendor#350, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).model, true, false) AS model#351, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborFqdn, true, false) AS neighborFqdn#352, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborIfName, true, false) AS neighborIfName#353, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborIp, true, false) AS neighborIp#354, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifIndex AS ifIndex#355, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).cpuType, true, false) AS cpuType#356, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).cpuName, true, false) AS cpuName#357, ... 8 more fields]
                                                      +- FlatMapGroupsWithState yet.another.company.snmp.dataframe.SnmpDatasetUtils$$$Lambda$1601/1736781209@1f8deca0, cast(value#268 as string).toString, newInstance(class yet.another.company.snmp.dto.RichSnmpRecord), [value#268], [deviceId#242, deviceName#243, deviceIp#244, objectName#245, ifDesc#246, indicatorName#247, value#248, time#249L, pluginName#250, vendor#251, model#252, neighborFqdn#253, neighborIfName#254, neighborIp#255, ifIndex#256, id#257, sourceType#258, cpuType#259, cpuName#260, ifName#261, division#262, metric#263, status#264, tags#265, historyRecords#266], obj#333: yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, class[uuid[0]: string, id[0]: string, currValue[0]: decimal(38,18), time[0]: bigint, updated[0]: boolean, isDiff[0]: boolean, diffValue[0]: double, deviceName[0]: string, deviceIp[0]: string, ifName[0]: string, ifDesc[0]: string, indicatorName[0]: string, objectName[0]: string, division[0]: string, sourceType[0]: string, prevTime[0]: bigint, vendor[0]: string, model[0]: string, neighborFqdn[0]: string, neighborIfName[0]: string, neighborIp[0]: string, ifIndex[0]: int, cpuType[0]: string, cpuName[0]: string, status[0]: struct<underMaintenance:boolean,inInventory:boolean,neighborUnderMaintenance:boolean>, metric[0]: string, tags[0]: map<string,string>, sysId[0]: string, rate[0]: double, history[0]: array<struct<time:bigint,value:decimal(38,18)>>, metaInfo[0]: struct<batchId:bigint>, historyRecords[0]: array<struct<ruleId:string,ruleDescription:string,workflowId:string,className:string,nodeIp:string,eventTime:string>>], Update, false, NoTimeout
                                                         +- AppendColumns yet.another.company.snmp.dataframe.SnmpDatasetUtils$$$Lambda$1599/1758533504@22f04839, class yet.another.company.snmp.dto.RichSnmpRecord, [StructField(deviceId,IntegerType,false), StructField(deviceName,StringType,true), StructField(deviceIp,StringType,true), StructField(objectName,StringType,true), StructField(ifDesc,StringType,true), StructField(indicatorName,StringType,true), StructField(value,DecimalType(38,18),true), StructField(time,LongType,false), StructField(pluginName,StringType,true), StructField(vendor,StringType,true), StructField(model,StringType,true), StructField(neighborFqdn,StringType,true), StructField(neighborIfName,StringType,true), StructField(neighborIp,StringType,true), StructField(ifIndex,IntegerType,false), StructField(id,StringType,true), StructField(sourceType,StringType,true), StructField(cpuType,StringType,true), StructField(cpuName,StringType,true), StructField(ifName,StringType,true), StructField(division,StringType,true), StructField(metric,StringType,true), StructField(status,StructType(StructField(underMaintenance,BooleanType,false), StructField(inInventory,BooleanType,false), StructField(neighborUnderMaintenance,BooleanType,true)),true), StructField(tags,MapType(StringType,StringType,true),true), StructField(historyRecords,ArrayType(StructType(StructField(ruleId,StringType,true), StructField(ruleDescription,StringType,true), StructField(workflowId,StringType,true), StructField(className,StringType,true), StructField(nodeIp,StringType,true), StructField(eventTime,StringType,true)),true),true)], newInstance(class yet.another.company.snmp.dto.RichSnmpRecord), [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, java.lang.String, true], true, false) AS value#268]
                                                            +- SerializeFromObject [knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).deviceId AS deviceId#242, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).deviceName, true, false) AS deviceName#243, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).deviceIp, true, false) AS deviceIp#244, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).objectName, true, false) AS objectName#245, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).ifDesc, true, false) AS ifDesc#246, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).indicatorName, true, false) AS indicatorName#247, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).value AS value#248, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).time AS time#249L, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).pluginName, true, false) AS pluginName#250, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).vendor, true, false) AS vendor#251, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).model, true, false) AS model#252, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).neighborFqdn, true, false) AS neighborFqdn#253, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).neighborIfName, true, false) AS neighborIfName#254, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).neighborIp, true, false) AS neighborIp#255, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).ifIndex AS ifIndex#256, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).id, true, false) AS id#257, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).sourceType, true, false) AS sourceType#258, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).cpuType, true, false) AS cpuType#259, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).cpuName, true, false) AS cpuName#260, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).ifName, true, false) AS ifName#261, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).division, true, false) AS division#262, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).metric, true, false) AS metric#263, if (isnull(knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).status)) null else named_struct(underMaintenance, knownnotnull(knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).status).underMaintenance, inInventory, knownnotnull(knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).status).inInventory, neighborUnderMaintenance, unwrapoption(BooleanType, knownnotnull(knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).status).neighborUnderMaintenance)) AS status#264, externalmaptocatalyst(lambdavariable(ExternalMapToCatalyst_key, ObjectType(class java.lang.String), true, 4), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, lambdavariable(ExternalMapToCatalyst_key, ObjectType(class java.lang.String), true, 4), true, false), lambdavariable(ExternalMapToCatalyst_value, ObjectType(class java.lang.String), true, 5), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, lambdavariable(ExternalMapToCatalyst_value, ObjectType(class java.lang.String), true, 5), true, false), knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).tags) AS tags#265, mapobjects(lambdavariable(MapObject, ObjectType(class yet.another.company.core.dto.ProcessingPoint), true, 6), if (isnull(lambdavariable(MapObject, ObjectType(class yet.another.company.core.dto.ProcessingPoint), true, 6))) null else named_struct(ruleId, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(lambdavariable(MapObject, ObjectType(class yet.another.company.core.dto.ProcessingPoint), true, 6)).ruleId, true, false), ruleDescription, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(lambdavariable(MapObject, ObjectType(class yet.another.company.core.dto.ProcessingPoint), true, 6)).ruleDescription, true, false), workflowId, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(lambdavariable(MapObject, ObjectType(class yet.another.company.core.dto.ProcessingPoint), true, 6)).workflowId, true, false), className, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(lambdavariable(MapObject, ObjectType(class yet.another.company.core.dto.ProcessingPoint), true, 6)).className, true, false), nodeIp, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(lambdavariable(MapObject, ObjectType(class yet.another.company.core.dto.ProcessingPoint), true, 6)).nodeIp, true, false), eventTime, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(lambdavariable(MapObject, ObjectType(class yet.another.company.core.dto.ProcessingPoint), true, 6)).eventTime, true, false)), knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).historyRecords, None) AS historyRecords#266]
                                                               +- MapPartitions org.apache.spark.sql.Dataset$$Lambda$1390/1145586277@3e61d9b2, obj#241: yet.another.company.snmp.dto.RichSnmpRecord
                                                                  +- DeserializeToObject newInstance(class yet.another.company.snmp.dto.RichSnmpRecord), obj#240: yet.another.company.snmp.dto.RichSnmpRecord
                                                                     +- SerializeFromObject [knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).deviceId AS deviceId#190, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).deviceName, true, false) AS deviceName#191, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).deviceIp, true, false) AS deviceIp#192, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).objectName, true, false) AS objectName#193, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).ifDesc, true, false) AS ifDesc#194, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).indicatorName, true, false) AS indicatorName#195, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).value AS value#196, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).time AS time#197L, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).pluginName, true, false) AS pluginName#198, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).vendor, true, false) AS vendor#199, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).model, true, false) AS model#200, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).neighborFqdn, true, false) AS neighborFqdn#201, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).neighborIfName, true, false) AS neighborIfName#202, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).neighborIp, true, false) AS neighborIp#203, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).ifIndex AS ifIndex#204, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).id, true, false) AS id#205, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).sourceType, true, false) AS sourceType#206, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).cpuType, true, false) AS cpuType#207, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).cpuName, true, false) AS cpuName#208, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).ifName, true, false) AS ifName#209, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).division, true, false) AS division#210, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).metric, true, false) AS metric#211, if (isnull(knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).status)) null else named_struct(underMaintenance, knownnotnull(knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).status).underMaintenance, inInventory, knownnotnull(knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).status).inInventory, neighborUnderMaintenance, unwrapoption(BooleanType, knownnotnull(knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).status).neighborUnderMaintenance)) AS status#212, externalmaptocatalyst(lambdavariable(ExternalMapToCatalyst_key, ObjectType(class java.lang.String), true, 1), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, lambdavariable(ExternalMapToCatalyst_key, ObjectType(class java.lang.String), true, 1), true, false), lambdavariable(ExternalMapToCatalyst_value, ObjectType(class java.lang.String), true, 2), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, lambdavariable(ExternalMapToCatalyst_value, ObjectType(class java.lang.String), true, 2), true, false), knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).tags) AS tags#213, mapobjects(lambdavariable(MapObject, ObjectType(class yet.another.company.core.dto.ProcessingPoint), true, 3), if (isnull(lambdavariable(MapObject, ObjectType(class yet.another.company.core.dto.ProcessingPoint), true, 3))) null else named_struct(ruleId, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(lambdavariable(MapObject, ObjectType(class yet.another.company.core.dto.ProcessingPoint), true, 3)).ruleId, true, false), ruleDescription, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(lambdavariable(MapObject, ObjectType(class yet.another.company.core.dto.ProcessingPoint), true, 3)).ruleDescription, true, false), workflowId, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(lambdavariable(MapObject, ObjectType(class yet.another.company.core.dto.ProcessingPoint), true, 3)).workflowId, true, false), className, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(lambdavariable(MapObject, ObjectType(class yet.another.company.core.dto.ProcessingPoint), true, 3)).className, true, false), nodeIp, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(lambdavariable(MapObject, ObjectType(class yet.another.company.core.dto.ProcessingPoint), true, 3)).nodeIp, true, false), eventTime, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(lambdavariable(MapObject, ObjectType(class yet.another.company.core.dto.ProcessingPoint), true, 3)).eventTime, true, false)), knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).historyRecords, None) AS historyRecords#214]
                                                                        +- MapPartitions org.apache.spark.sql.Dataset$$Lambda$1390/1145586277@4dcc59b2, obj#189: yet.another.company.snmp.dto.RichSnmpRecord
                                                                           +- DeserializeToObject newInstance(class yet.another.company.snmp.streaming.InputSevOneRecord), obj#188: yet.another.company.snmp.streaming.InputSevOneRecord
                                                                              +- Union
                                                                                 :- TypedFilter yet.another.company.snmp.streaming.SnmpStreaming$$$Lambda$1407/1698415889@6ffc3dac, class yet.another.company.snmp.streaming.InputSevOneRecord, [StructField(deviceId,IntegerType,false), StructField(deviceName,StringType,true), StructField(deviceIp,StringType,true), StructField(peerId,IntegerType,false), StructField(objectId,IntegerType,false), StructField(objectName,StringType,true), StructField(objectDesc,StringType,true), StructField(pluginId,IntegerType,false), StructField(pluginName,StringType,true), StructField(indicatorId,IntegerType,false), StructField(indicatorName,StringType,true), StructField(format,IntegerType,false), StructField(value,StringType,true), StructField(time,DoubleType,false), StructField(clusterName,StringType,true), StructField(peerIp,StringType,true)], newInstance(class yet.another.company.snmp.streaming.InputSevOneRecord)
                                                                                 :  +- SerializeFromObject [knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).deviceId AS deviceId#40, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).deviceName, true, false) AS deviceName#41, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).deviceIp, true, false) AS deviceIp#42, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).peerId AS peerId#43, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).objectId AS objectId#44, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).objectName, true, false) AS objectName#45, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).objectDesc, true, false) AS objectDesc#46, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).pluginId AS pluginId#47, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).pluginName, true, false) AS pluginName#48, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).indicatorId AS indicatorId#49, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).indicatorName, true, false) AS indicatorName#50, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).format AS format#51, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).value, true, false) AS value#52, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).time AS time#53, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).clusterName, true, false) AS clusterName#54, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).peerIp, true, false) AS peerIp#55]
                                                                                 :     +- MapPartitions org.apache.spark.sql.Dataset$$Lambda$1390/1145586277@252f7a0a, obj#39: yet.another.company.snmp.streaming.InputSevOneRecord
                                                                                 :        +- DeserializeToObject cast(value#8 as binary), obj#38: binary
                                                                                 :           +- Project [value#8]
                                                                                 :              +- StreamingDataSourceV2Relation [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan@63d7838, KafkaV2[Subscribe[sevone-spdb]]
                                                                                 +- TypedFilter yet.another.company.snmp.streaming.SnmpStreaming$$$Lambda$1407/1698415889@6ffc3dac, class yet.another.company.snmp.streaming.InputSevOneRecord, [StructField(deviceId,IntegerType,false), StructField(deviceName,StringType,true), StructField(deviceIp,StringType,true), StructField(peerId,IntegerType,false), StructField(objectId,IntegerType,false), StructField(objectName,StringType,true), StructField(objectDesc,StringType,true), StructField(pluginId,IntegerType,false), StructField(pluginName,StringType,true), StructField(indicatorId,IntegerType,false), StructField(indicatorName,StringType,true), StructField(format,IntegerType,false), StructField(value,StringType,true), StructField(time,DoubleType,false), StructField(clusterName,StringType,true), StructField(peerIp,StringType,true)], newInstance(class yet.another.company.snmp.streaming.InputSevOneRecord)
                                                                                    +- SerializeFromObject [knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).deviceId AS deviceId#113, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).deviceName, true, false) AS deviceName#114, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).deviceIp, true, false) AS deviceIp#115, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).peerId AS peerId#116, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).objectId AS objectId#117, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).objectName, true, false) AS objectName#118, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).objectDesc, true, false) AS objectDesc#119, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).pluginId AS pluginId#120, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).pluginName, true, false) AS pluginName#121, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).indicatorId AS indicatorId#122, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).indicatorName, true, false) AS indicatorName#123, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).format AS format#124, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).value, true, false) AS value#125, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).time AS time#126, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).clusterName, true, false) AS clusterName#127, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.streaming.InputSevOneRecord, true])).peerIp, true, false) AS peerIp#128]
                                                                                       +- MapPartitions org.apache.spark.sql.Dataset$$Lambda$1390/1145586277@5be5f16, obj#112: yet.another.company.snmp.streaming.InputSevOneRecord
                                                                                          +- DeserializeToObject cast(value#81 as binary), obj#111: binary
                                                                                             +- Project [value#81]
                                                                                                +- StreamingDataSourceV2Relation [key#80, value#81, topic#82, partition#83, offset#84L, timestamp#85, timestampType#86], org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan@46530e12, KafkaV2[Subscribe[sevone-spdb]]

	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:355)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:245)
Caused by: org.apache.spark.SparkException: Writing job aborted.
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:413)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:361)
	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:322)
	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:329)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:39)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:39)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:45)
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3664)
	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:2980)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3655)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:106)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:207)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:88)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3653)
	at org.apache.spark.sql.Dataset.collect(Dataset.scala:2980)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:576)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:106)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:207)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:88)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$15(MicroBatchExecution.scala:571)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:352)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:350)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:571)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:223)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:352)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:350)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:191)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:185)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:334)
	... 1 more
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 153 in stage 1.0 failed 4 times, most recent failure: Lost task 153.3 in stage 1.0 (TID 1026, ip-96-113-30-137.nest.r53.xcal.tv, executor 12): org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-183115326-96.113.30.153-1605540097659:blk_1073969993_240365 file=/snmp-checkpoints/state/0/153/539.snapshot
	at org.apache.hadoop.hdfs.DFSInputStream.refetchLocations(DFSInputStream.java:879)
	at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:862)
	at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:841)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:567)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:757)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:829)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at net.jpountz.lz4.LZ4BlockInputStream.tryReadFully(LZ4BlockInputStream.java:269)
	at net.jpountz.lz4.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:190)
	at net.jpountz.lz4.LZ4BlockInputStream.read(LZ4BlockInputStream.java:142)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.readSnapshotFile(HDFSBackedStateStoreProvider.scala:528)
	at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$3(HDFSBackedStateStoreProvider.scala:376)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$2(HDFSBackedStateStoreProvider.scala:376)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:561)
	at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.loadMap(HDFSBackedStateStoreProvider.scala:356)
	at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getStore(HDFSBackedStateStoreProvider.scala:204)
	at org.apache.spark.sql.execution.streaming.state.StateStore$.get(StateStore.scala:371)
	at org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:90)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2175)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2124)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2123)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2123)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:990)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:990)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:990)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2355)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2304)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2293)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:792)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:382)
	... 37 more
Caused by: org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-183115326-96.113.30.153-1605540097659:blk_1073969993_240365 file=/snmp-checkpoints/state/0/153/539.snapshot
	at org.apache.hadoop.hdfs.DFSInputStream.refetchLocations(DFSInputStream.java:879)
	at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:862)
	at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:841)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:567)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:757)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:829)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at net.jpountz.lz4.LZ4BlockInputStream.tryReadFully(LZ4BlockInputStream.java:269)
	at net.jpountz.lz4.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:190)
	at net.jpountz.lz4.LZ4BlockInputStream.read(LZ4BlockInputStream.java:142)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.readSnapshotFile(HDFSBackedStateStoreProvider.scala:528)
	at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$3(HDFSBackedStateStoreProvider.scala:376)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$2(HDFSBackedStateStoreProvider.scala:376)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:561)
	at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.loadMap(HDFSBackedStateStoreProvider.scala:356)
	at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getStore(HDFSBackedStateStoreProvider.scala:204)
	at org.apache.spark.sql.execution.streaming.state.StateStore$.get(StateStore.scala:371)
	at org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:90)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	 ApplicationMaster host: ip-96-113-30-250.nest.r53.xcal.tv
	 ApplicationMaster RPC port: 35267
	 queue: default
	 start time: 1605793957502
	 final status: FAILED
	 tracking URL: http://ip-96-113-30-153.nest.r53.xcal.tv:20888/proxy/application_1605540135355_0076/
	 user: hadoop
20/11/19 13:53:50 ERROR Client: Application diagnostics message: Diagnostic messages truncated, showing last 65536 chars out of 91225:
...org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborIfName, true, false) AS neighborIfName#683, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborIp, true, false) AS neighborIp#684, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifIndex AS ifIndex#685, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).cpuType, true, false) AS cpuType#686, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).cpuName, true, false) AS cpuName#687, ... 8 more fields]
                        +- MapElements yet.another.company.snmp.dataframe.SnmpDatasetUtils$$$Lambda$1622/839345003@411b1647, class yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, [StructField(uuid,StringType,true), StructField(id,StringType,true), StructField(currValue,DecimalType(38,18),true), StructField(time,LongType,false), StructField(updated,BooleanType,false), StructField(isDiff,BooleanType,false), StructField(diffValue,DoubleType,false), StructField(deviceName,StringType,true), StructField(deviceIp,StringType,true), StructField(ifName,StringType,true), StructField(ifDesc,StringType,true), StructField(indicatorName,StringType,true), StructField(objectName,StringType,true), StructField(division,StringType,true), StructField(sourceType,StringType,true), StructField(prevTime,LongType,false), StructField(vendor,StringType,true), StructField(model,StringType,true), StructField(neighborFqdn,StringType,true), StructField(neighborIfName,StringType,true), StructField(neighborIp,StringType,true), StructField(ifIndex,IntegerType,false), StructField(cpuType,StringType,true), StructField(cpuName,StringType,true), ... 8 more fields], obj#663: yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState
                           +- DeserializeToObject newInstance(class yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState), obj#662: yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState
                              +- SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).uuid, true, false) AS uuid#565, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).id, true, false) AS id#566, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).currValue AS currValue#567, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).time AS time#568L, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).updated AS updated#569, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).isDiff AS isDiff#570, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).diffValue AS diffValue#571, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).deviceName, true, false) AS deviceName#572, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).deviceIp, true, false) AS deviceIp#573, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifName, true, false) AS ifName#574, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifDesc, true, false) AS ifDesc#575, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).indicatorName, true, false) AS indicatorName#576, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).objectName, true, false) AS objectName#577, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).division, true, false) AS division#578, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).sourceType, true, false) AS sourceType#579, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).prevTime AS prevTime#580L, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).vendor, true, false) AS vendor#581, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).model, true, false) AS model#582, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborFqdn, true, false) AS neighborFqdn#583, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborIfName, true, false) AS neighborIfName#584, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborIp, true, false) AS neighborIp#585, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifIndex AS ifIndex#586, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).cpuType, true, false) AS cpuType#587, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).cpuName, true, false) AS cpuName#588, ... 8 more fields]
                                 +- MapElements yet.another.company.core.metrics.LongValStatsAccumulator$$Lambda$1620/469523646@3fafe025, class yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, [StructField(uuid,StringType,true), StructField(id,StringType,true), StructField(currValue,DecimalType(38,18),true), StructField(time,LongType,false), StructField(updated,BooleanType,false), StructField(isDiff,BooleanType,false), StructField(diffValue,DoubleType,false), StructField(deviceName,StringType,true), StructField(deviceIp,StringType,true), StructField(ifName,StringType,true), StructField(ifDesc,StringType,true), StructField(indicatorName,StringType,true), StructField(objectName,StringType,true), StructField(division,StringType,true), StructField(sourceType,StringType,true), StructField(prevTime,LongType,false), StructField(vendor,StringType,true), StructField(model,StringType,true), StructField(neighborFqdn,StringType,true), StructField(neighborIfName,StringType,true), StructField(neighborIp,StringType,true), StructField(ifIndex,IntegerType,false), StructField(cpuType,StringType,true), StructField(cpuName,StringType,true), ... 8 more fields], obj#564: yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState
                                    +- DeserializeToObject newInstance(class yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState), obj#563: yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState
                                       +- TypedFilter yet.another.company.snmp.dataframe.SnmpDatasetUtils$$$Lambda$1617/154986516@6487ffa7, class yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, [StructField(uuid,StringType,true), StructField(id,StringType,true), StructField(currValue,DecimalType(38,18),true), StructField(time,LongType,false), StructField(updated,BooleanType,false), StructField(isDiff,BooleanType,false), StructField(diffValue,DoubleType,false), StructField(deviceName,StringType,true), StructField(deviceIp,StringType,true), StructField(ifName,StringType,true), StructField(ifDesc,StringType,true), StructField(indicatorName,StringType,true), StructField(objectName,StringType,true), StructField(division,StringType,true), StructField(sourceType,StringType,true), StructField(prevTime,LongType,false), StructField(vendor,StringType,true), StructField(model,StringType,true), StructField(neighborFqdn,StringType,true), StructField(neighborIfName,StringType,true), StructField(neighborIp,StringType,true), StructField(ifIndex,IntegerType,false), StructField(cpuType,StringType,true), StructField(cpuName,StringType,true), ... 8 more fields], newInstance(class yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState)
                                          +- SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).uuid, true, false) AS uuid#433, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).id, true, false) AS id#434, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).currValue AS currValue#435, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).time AS time#436L, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).updated AS updated#437, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).isDiff AS isDiff#438, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).diffValue AS diffValue#439, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).deviceName, true, false) AS deviceName#440, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).deviceIp, true, false) AS deviceIp#441, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifName, true, false) AS ifName#442, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifDesc, true, false) AS ifDesc#443, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).indicatorName, true, false) AS indicatorName#444, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).objectName, true, false) AS objectName#445, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).division, true, false) AS division#446, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).sourceType, true, false) AS sourceType#447, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).prevTime AS prevTime#448L, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).vendor, true, false) AS vendor#449, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).model, true, false) AS model#450, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborFqdn, true, false) AS neighborFqdn#451, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborIfName, true, false) AS neighborIfName#452, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborIp, true, false) AS neighborIp#453, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifIndex AS ifIndex#454, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).cpuType, true, false) AS cpuType#455, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).cpuName, true, false) AS cpuName#456, ... 8 more fields]
                                             +- MapElements yet.another.company.snmp.dataframe.SnmpDatasetUtils$$$Lambda$1616/1774441263@7f928ab8, class yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, [StructField(uuid,StringType,true), StructField(id,StringType,true), StructField(currValue,DecimalType(38,18),true), StructField(time,LongType,false), StructField(updated,BooleanType,false), StructField(isDiff,BooleanType,false), StructField(diffValue,DoubleType,false), StructField(deviceName,StringType,true), StructField(deviceIp,StringType,true), StructField(ifName,StringType,true), StructField(ifDesc,StringType,true), StructField(indicatorName,StringType,true), StructField(objectName,StringType,true), StructField(division,StringType,true), StructField(sourceType,StringType,true), StructField(prevTime,LongType,false), StructField(vendor,StringType,true), StructField(model,StringType,true), StructField(neighborFqdn,StringType,true), StructField(neighborIfName,StringType,true), StructField(neighborIp,StringType,true), StructField(ifIndex,IntegerType,false), StructField(cpuType,StringType,true), StructField(cpuName,StringType,true), ... 8 more fields], obj#432: yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState
                                                +- DeserializeToObject newInstance(class yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState), obj#431: yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState
                                                   +- SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).uuid, true, false) AS uuid#334, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).id, true, false) AS id#335, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).currValue AS currValue#336, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).time AS time#337L, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).updated AS updated#338, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).isDiff AS isDiff#339, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).diffValue AS diffValue#340, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).deviceName, true, false) AS deviceName#341, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).deviceIp, true, false) AS deviceIp#342, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifName, true, false) AS ifName#343, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifDesc, true, false) AS ifDesc#344, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).indicatorName, true, false) AS indicatorName#345, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).objectName, true, false) AS objectName#346, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).division, true, false) AS division#347, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).sourceType, true, false) AS sourceType#348, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).prevTime AS prevTime#349L, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).vendor, true, false) AS vendor#350, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).model, true, false) AS model#351, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborFqdn, true, false) AS neighborFqdn#352, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborIfName, true, false) AS neighborIfName#353, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).neighborIp, true, false) AS neighborIp#354, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).ifIndex AS ifIndex#355, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).cpuType, true, false) AS cpuType#356, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, true])).cpuName, true, false) AS cpuName#357, ... 8 more fields]
                                                      +- FlatMapGroupsWithState yet.another.company.snmp.dataframe.SnmpDatasetUtils$$$Lambda$1601/1736781209@1f8deca0, cast(value#268 as string).toString, newInstance(class yet.another.company.snmp.dto.RichSnmpRecord), [value#268], [deviceId#242, deviceName#243, deviceIp#244, objectName#245, ifDesc#246, indicatorName#247, value#248, time#249L, pluginName#250, vendor#251, model#252, neighborFqdn#253, neighborIfName#254, neighborIp#255, ifIndex#256, id#257, sourceType#258, cpuType#259, cpuName#260, ifName#261, division#262, metric#263, status#264, tags#265, historyRecords#266], obj#333: yet.another.company.snmp.dataframe.SnmpDataFrameUtils$SnmpAggState, class[uuid[0]: string, id[0]: string, currValue[0]: decimal(38,18), time[0]: bigint, updated[0]: boolean, isDiff[0]: boolean, diffValue[0]: double, deviceName[0]: string, deviceIp[0]: string, ifName[0]: string, ifDesc[0]: string, indicatorName[0]: string, objectName[0]: string, division[0]: string, sourceType[0]: string, prevTime[0]: bigint, vendor[0]: string, model[0]: string, neighborFqdn[0]: string, neighborIfName[0]: string, neighborIp[0]: string, ifIndex[0]: int, cpuType[0]: string, cpuName[0]: string, status[0]: struct<underMaintenance:boolean,inInventory:boolean,neighborUnderMaintenance:boolean>, metric[0]: string, tags[0]: map<string,string>, sysId[0]: string, rate[0]: double, history[0]: array<struct<time:bigint,value:decimal(38,18)>>, metaInfo[0]: struct<batchId:bigint>, historyRecords[0]: array<struct<ruleId:string,ruleDescription:string,workflowId:string,className:string,nodeIp:string,eventTime:string>>], Update, false, NoTimeout
                                                         +- AppendColumns yet.another.company.snmp.dataframe.SnmpDatasetUtils$$$Lambda$1599/1758533504@22f04839, class yet.another.company.snmp.dto.RichSnmpRecord, [StructField(deviceId,IntegerType,false), StructField(deviceName,StringType,true), StructField(deviceIp,StringType,true), StructField(objectName,StringType,true), StructField(ifDesc,StringType,true), StructField(indicatorName,StringType,true), StructField(value,DecimalType(38,18),true), StructField(time,LongType,false), StructField(pluginName,StringType,true), StructField(vendor,StringType,true), StructField(model,StringType,true), StructField(neighborFqdn,StringType,true), StructField(neighborIfName,StringType,true), StructField(neighborIp,StringType,true), StructField(ifIndex,IntegerType,false), StructField(id,StringType,true), StructField(sourceType,StringType,true), StructField(cpuType,StringType,true), StructField(cpuName,StringType,true), StructField(ifName,StringType,true), StructField(division,StringType,true), StructField(metric,StringType,true), StructField(status,StructType(StructField(underMaintenance,BooleanType,false), StructField(inInventory,BooleanType,false), StructField(neighborUnderMaintenance,BooleanType,true)),true), StructField(tags,MapType(StringType,StringType,true),true), StructField(historyRecords,ArrayType(StructType(StructField(ruleId,StringType,true), StructField(ruleDescription,StringType,true), StructField(workflowId,StringType,true), StructField(className,StringType,true), StructField(nodeIp,StringType,true), StructField(eventTime,StringType,true)),true),true)], newInstance(class yet.another.company.snmp.dto.RichSnmpRecord), [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, java.lang.String, true], true, false) AS value#268]
                                                            +- SerializeFromObject [knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).deviceId AS deviceId#242, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).deviceName, true, false) AS deviceName#243, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).deviceIp, true, false) AS deviceIp#244, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).objectName, true, false) AS objectName#245, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).ifDesc, true, false) AS ifDesc#246, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).indicatorName, true, false) AS indicatorName#247, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).value AS value#248, knownnotnull(assertnotnull(input[0, yet.another.company.snmp.dto.RichSnmpRecord, true])).time AS time#249L, staticinvoke(class org.apCommand exiting with ret '1'
